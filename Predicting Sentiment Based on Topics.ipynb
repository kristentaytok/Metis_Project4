{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:11:03.036615Z",
     "start_time": "2019-11-14T19:10:59.545815Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "\n",
    "#Vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Dimensionality Reduction\n",
    "from sklearn.decomposition import TruncatedSVD #LSA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from gensim import corpora, models, similarities, matutils #LDA\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Bayes Optimization Parameter Tuner\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#For Handling Imbalanced Data for Classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import numpy as np\n",
    "\n",
    "#For Classification\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize  #not needed b/c filtered out neutral ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:11:03.432002Z",
     "start_time": "2019-11-14T19:11:03.037700Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('full_df_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:30:56.454201Z",
     "start_time": "2019-11-13T21:30:56.431839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>movie</th>\n",
       "      <th>review_site</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>review_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Disney, WHAT. HAVE. YOU. DONE Just to be clea...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[disney, clear, time, favorite, movie, not, st...</td>\n",
       "      <td>disney clear time favorite movie not stress en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No soul. The original Lion King is one of my ...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[no, soul, original, lion, king, favorite, mov...</td>\n",
       "      <td>no soul original lion king favorite movie time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Seriously? So anyone else notice it has a hig...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[seriously, notice, high, score, 7.5, rating, ...</td>\n",
       "      <td>seriously notice high score 7.5 rating not str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Overrated and way too much spotlight on beyon...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[overrated, way, spotlight, beyonce, lion, kin...</td>\n",
       "      <td>overrated way spotlight beyonce lion king only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Terrible acting!! Doesn't compare to the orig...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[terrible, act, not, compare, original, love, ...</td>\n",
       "      <td>terrible act not compare original love origina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>A magically wonderful film filled with adventu...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>[magically, wonderful, film, fill, adventure, ...</td>\n",
       "      <td>magically wonderful film fill adventure fantas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3041</td>\n",
       "      <td>Disney has overdid the faithfulness of their o...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[disney, overdo, faithfulness, animate, classi...</td>\n",
       "      <td>disney overdo faithfulness animate classic pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3042</td>\n",
       "      <td>Magic....that's about right. A re-tell of the ...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[magic, ...., right, tell, original, disney, m...</td>\n",
       "      <td>magic .... right tell original disney movie li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3043</td>\n",
       "      <td>A good movie that sets it apart from the origi...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[good, movie, set, apart, original, story, cin...</td>\n",
       "      <td>good movie set apart original story cinderella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3044</td>\n",
       "      <td>It's the Cinderella you know but it's really d...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[cinderella, know, delightfully, charm]</td>\n",
       "      <td>cinderella know delightfully charm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51512 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_text       movie  \\\n",
       "0      Disney, WHAT. HAVE. YOU. DONE Just to be clea...    lionking   \n",
       "1      No soul. The original Lion King is one of my ...    lionking   \n",
       "5      Seriously? So anyone else notice it has a hig...    lionking   \n",
       "6      Overrated and way too much spotlight on beyon...    lionking   \n",
       "8      Terrible acting!! Doesn't compare to the orig...    lionking   \n",
       "...                                                 ...         ...   \n",
       "3040  A magically wonderful film filled with adventu...  cinderella   \n",
       "3041  Disney has overdid the faithfulness of their o...  cinderella   \n",
       "3042  Magic....that's about right. A re-tell of the ...  cinderella   \n",
       "3043  A good movie that sets it apart from the origi...  cinderella   \n",
       "3044  It's the Cinderella you know but it's really d...  cinderella   \n",
       "\n",
       "         review_site  rating sentiment  \\\n",
       "0               imdb       1  negative   \n",
       "1               imdb       1  negative   \n",
       "5               imdb       1  negative   \n",
       "6               imdb       1  negative   \n",
       "8               imdb       1  negative   \n",
       "...              ...     ...       ...   \n",
       "3040  rottentomatoes       5  positive   \n",
       "3041  rottentomatoes       4  positive   \n",
       "3042  rottentomatoes       4  positive   \n",
       "3043  rottentomatoes       4  positive   \n",
       "3044  rottentomatoes       4  positive   \n",
       "\n",
       "                                          review_tokens  \\\n",
       "0     [disney, clear, time, favorite, movie, not, st...   \n",
       "1     [no, soul, original, lion, king, favorite, mov...   \n",
       "5     [seriously, notice, high, score, 7.5, rating, ...   \n",
       "6     [overrated, way, spotlight, beyonce, lion, kin...   \n",
       "8     [terrible, act, not, compare, original, love, ...   \n",
       "...                                                 ...   \n",
       "3040  [magically, wonderful, film, fill, adventure, ...   \n",
       "3041  [disney, overdo, faithfulness, animate, classi...   \n",
       "3042  [magic, ...., right, tell, original, disney, m...   \n",
       "3043  [good, movie, set, apart, original, story, cin...   \n",
       "3044            [cinderella, know, delightfully, charm]   \n",
       "\n",
       "                                       review_processed  \n",
       "0     disney clear time favorite movie not stress en...  \n",
       "1     no soul original lion king favorite movie time...  \n",
       "5     seriously notice high score 7.5 rating not str...  \n",
       "6     overrated way spotlight beyonce lion king only...  \n",
       "8     terrible act not compare original love origina...  \n",
       "...                                                 ...  \n",
       "3040  magically wonderful film fill adventure fantas...  \n",
       "3041  disney overdo faithfulness animate classic pro...  \n",
       "3042  magic .... right tell original disney movie li...  \n",
       "3043  good movie set apart original story cinderella...  \n",
       "3044                 cinderella know delightfully charm  \n",
       "\n",
       "[51512 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:11:10.132088Z",
     "start_time": "2019-11-14T19:11:10.087182Z"
    }
   },
   "outputs": [],
   "source": [
    "#change sentiment to numbers as classifiers \n",
    "full_df['sentiment'].replace('positive',1,inplace=True)\n",
    "full_df['sentiment'].replace('negative',0,inplace=True)\n",
    "full_df = full_df[(full_df.sentiment != 'neutral')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:11:11.858631Z",
     "start_time": "2019-11-14T19:11:11.847362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    38656\n",
       "0     5941\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split for Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:11:15.001543Z",
     "start_time": "2019-11-14T19:11:14.988009Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_df.drop(['rating','review_text','movie','review_site','review_text','sentiment', 'review_tokens'], axis=1), full_df['sentiment'], test_size=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:30:56.514479Z",
     "start_time": "2019-11-13T21:30:56.507337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    30928\n",
       "0     4749\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#very imbalanced--many positive reviews \n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:48:42.648537Z",
     "start_time": "2019-11-13T21:48:31.267114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35677, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = X_train.iloc[:,0].tolist() #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_LSA = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('LSA', TruncatedSVD(n_components=5,random_state=10, n_iter=10))])\n",
    "TFIDF_LSA = Pipeline_TFIDF_LSA.fit_transform(corpus)\n",
    "\n",
    "#Need to normalize before classifying to predict positive vs. negative sentiment\n",
    "X_train_TFIDF_LSA_normalized = normalize(TFIDF_LSA)\n",
    "X_train_TFIDF_LSA_normalized.shape\n",
    "\n",
    "#will need to normalize test set separately after doing same pipeline steps--> normalize --> stdscale TRANSFORM ONLY on test using fitted stdscale from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:59:19.798787Z",
     "start_time": "2019-11-13T21:59:08.413640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35677, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = X_train.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_NMF = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('NMF', NMF(n_components=5,random_state=10))])\n",
    "TFIDF_NMF = Pipeline_TFIDF_NMF.fit_transform(corpus)\n",
    "\n",
    "#Need to normalize before classifying to predict positive vs. negative sentiment\n",
    "X_train_TFIDF_NMF_normalized = normalize(TFIDF_NMF)\n",
    "X_train_TFIDF_NMF_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:00:39.076683Z",
     "start_time": "2019-11-13T22:00:39.057379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.618823</td>\n",
       "      <td>-0.164859</td>\n",
       "      <td>-0.299351</td>\n",
       "      <td>0.664971</td>\n",
       "      <td>-0.241003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.822541</td>\n",
       "      <td>-0.060692</td>\n",
       "      <td>-0.362104</td>\n",
       "      <td>0.270914</td>\n",
       "      <td>-0.339454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.611932</td>\n",
       "      <td>-0.658959</td>\n",
       "      <td>0.214088</td>\n",
       "      <td>0.254041</td>\n",
       "      <td>-0.284503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.935221</td>\n",
       "      <td>-0.263925</td>\n",
       "      <td>-0.099187</td>\n",
       "      <td>-0.148634</td>\n",
       "      <td>-0.154195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.574973</td>\n",
       "      <td>0.252747</td>\n",
       "      <td>-0.486537</td>\n",
       "      <td>0.601867</td>\n",
       "      <td>0.081013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35672</td>\n",
       "      <td>0.506348</td>\n",
       "      <td>0.094750</td>\n",
       "      <td>-0.185527</td>\n",
       "      <td>0.604422</td>\n",
       "      <td>0.578696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35673</td>\n",
       "      <td>0.810796</td>\n",
       "      <td>0.031377</td>\n",
       "      <td>-0.382283</td>\n",
       "      <td>0.282402</td>\n",
       "      <td>-0.340197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35674</td>\n",
       "      <td>0.630164</td>\n",
       "      <td>-0.597997</td>\n",
       "      <td>0.118572</td>\n",
       "      <td>0.354982</td>\n",
       "      <td>-0.324379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35675</td>\n",
       "      <td>0.840179</td>\n",
       "      <td>-0.361880</td>\n",
       "      <td>0.176981</td>\n",
       "      <td>-0.030525</td>\n",
       "      <td>-0.361784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35676</td>\n",
       "      <td>0.822169</td>\n",
       "      <td>0.430438</td>\n",
       "      <td>0.287584</td>\n",
       "      <td>-0.206154</td>\n",
       "      <td>-0.116433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35677 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic0    topic1    topic2    topic3    topic4\n",
       "0      0.618823 -0.164859 -0.299351  0.664971 -0.241003\n",
       "1      0.822541 -0.060692 -0.362104  0.270914 -0.339454\n",
       "2      0.611932 -0.658959  0.214088  0.254041 -0.284503\n",
       "3      0.935221 -0.263925 -0.099187 -0.148634 -0.154195\n",
       "4      0.574973  0.252747 -0.486537  0.601867  0.081013\n",
       "...         ...       ...       ...       ...       ...\n",
       "35672  0.506348  0.094750 -0.185527  0.604422  0.578696\n",
       "35673  0.810796  0.031377 -0.382283  0.282402 -0.340197\n",
       "35674  0.630164 -0.597997  0.118572  0.354982 -0.324379\n",
       "35675  0.840179 -0.361880  0.176981 -0.030525 -0.361784\n",
       "35676  0.822169  0.430438  0.287584 -0.206154 -0.116433\n",
       "\n",
       "[35677 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_TFIDF_LSA = pd.DataFrame(X_train_TFIDF_LSA_normalized,columns=['topic1','topic2','topic3','topic4','topic5'])\n",
    "X_train_TFIDF_LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:01:24.090955Z",
     "start_time": "2019-11-13T22:01:24.076584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.469244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883068</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.568487</td>\n",
       "      <td>0.364562</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.737508</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.999926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.741145</td>\n",
       "      <td>0.602491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156908</td>\n",
       "      <td>0.251174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.083206</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35672</td>\n",
       "      <td>0.078081</td>\n",
       "      <td>0.144820</td>\n",
       "      <td>0.125787</td>\n",
       "      <td>0.890371</td>\n",
       "      <td>0.405398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35673</td>\n",
       "      <td>0.502497</td>\n",
       "      <td>0.390324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.771456</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35674</td>\n",
       "      <td>0.997735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067262</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35675</td>\n",
       "      <td>0.915575</td>\n",
       "      <td>0.344054</td>\n",
       "      <td>0.208201</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35676</td>\n",
       "      <td>0.227692</td>\n",
       "      <td>0.741813</td>\n",
       "      <td>0.629399</td>\n",
       "      <td>0.041387</td>\n",
       "      <td>0.003663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35677 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic1    topic2    topic3    topic4    topic5\n",
       "0      0.469244  0.000000  0.000000  0.883068  0.000000\n",
       "1      0.568487  0.364562  0.000000  0.737508  0.000000\n",
       "2      0.999926  0.000000  0.010350  0.000000  0.006360\n",
       "3      0.741145  0.602491  0.000000  0.156908  0.251174\n",
       "4      0.083206  0.399400  0.000000  0.912993  0.000000\n",
       "...         ...       ...       ...       ...       ...\n",
       "35672  0.078081  0.144820  0.125787  0.890371  0.405398\n",
       "35673  0.502497  0.390324  0.000000  0.771456  0.000000\n",
       "35674  0.997735  0.000000  0.000000  0.067262  0.000000\n",
       "35675  0.915575  0.344054  0.208201  0.001029  0.000000\n",
       "35676  0.227692  0.741813  0.629399  0.041387  0.003663\n",
       "\n",
       "[35677 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_TFIDF_NMF = pd.DataFrame(X_train_TFIDF_NMF_normalized,columns=['topic1','topic2','topic3','topic4','topic5'])\n",
    "X_train_TFIDF_NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:31:19.833686Z",
     "start_time": "2019-11-13T21:31:19.827278Z"
    }
   },
   "outputs": [],
   "source": [
    "#for some reason, y_train has datatype \"unknown\" --> have to convert to integer\n",
    "y_train = y_train.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:31:20.143592Z",
     "start_time": "2019-11-13T21:30:53.349Z"
    }
   },
   "source": [
    "# Bayes Optimization for Hyperparameter Tuning Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:50:47.855326Z",
     "start_time": "2019-11-13T22:29:02.772366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [21:45<00:00,  2.61s/it, best loss: 0.2200161293137115]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.2200161293137115,\n",
       " 'params': {'C': 10, 'penalty': 'l1', 'solver': 'saga'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train_TFIDF_LSA = StandardScaler().fit_transform(X_train_TFIDF_LSA)\n",
    "\n",
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, class_weight='balanced', random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_X_train_TFIDF_LSA, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T01:16:11.773237Z",
     "start_time": "2019-11-14T01:16:00.336550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5233835789469081\n",
      "0.5563100961538461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.61245935,  0.08845889,  0.64760042,  0.55574021,  0.09554249]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "###SUBTRAIN\n",
    "subtrain_corpus = X_subtrain.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_LSA_subtrain = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('LSA', TruncatedSVD(n_components=5,random_state=10,n_iter=10))])\n",
    "TFIDF_LSA_subtrain = Pipeline_TFIDF_LSA_subtrain.fit_transform(subtrain_corpus)\n",
    "\n",
    "X_subtrain_TFIDF_LSA_normalized = normalize(TFIDF_LSA_subtrain)\n",
    "ss = StandardScaler()\n",
    "scaled_subtrain_TFIDF_LSA = ss.fit_transform(X_subtrain_TFIDF_LSA_normalized)\n",
    "\n",
    "\n",
    "\n",
    "###VALID\n",
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "valid_corpus = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_LSA_valid = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('LSA', TruncatedSVD(n_components=5,random_state=10,n_iter=10))])\n",
    "TFIDF_LSA_valid = Pipeline_TFIDF_LSA_valid.fit_transform(valid_corpus)\n",
    "\n",
    "X_valid_TFIDF_LSA_normalized = normalize(TFIDF_LSA_valid)\n",
    "scaled_valid_TFIDF_LSA = ss.transform(X_valid_TFIDF_LSA_normalized)\n",
    "\n",
    "logreg_classwt_LSA = LogisticRegression(C=10, penalty='l1', solver='saga', class_weight='balanced', random_state=41, max_iter=500).fit(scaled_subtrain_TFIDF_NMF, y_subtrain)\n",
    "y_pred_classwt_LSA_subtrain = logreg_classwt_LSA.predict(scaled_subtrain_TFIDF_LSA)\n",
    "y_pred_classwt_LSA_valid = logreg_classwt_LSA.predict(scaled_valid_TFIDF_LSA)\n",
    "\n",
    "print(roc_auc_score(y_subtrain, y_pred_classwt_LSA_subtrain))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_LSA_valid))\n",
    "logreg_classwt_LSA.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NMF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:27:34.078563Z",
     "start_time": "2019-11-13T22:23:55.010699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [03:39<00:00,  2.28it/s, best loss: 0.2346180047390709]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.2346180047390709,\n",
       " 'params': {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train_TFIDF_NMF = StandardScaler().fit_transform(X_train_TFIDF_NMF)\n",
    "\n",
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, class_weight='balanced', random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_X_train_TFIDF_NMF, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T01:16:22.577781Z",
     "start_time": "2019-11-14T01:16:11.774829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7086049627353526\n",
      "0.7035141941391941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.60523788,  0.08619265,  0.61950938,  0.53780974,  0.09247201]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "###SUBTRAIN\n",
    "subtrain_corpus = X_subtrain.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_NMF_subtrain = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('NMF', NMF(n_components=5,random_state=10))])\n",
    "TFIDF_NMF_subtrain = Pipeline_TFIDF_NMF_subtrain.fit_transform(subtrain_corpus)\n",
    "\n",
    "X_subtrain_TFIDF_NMF_normalized = normalize(TFIDF_NMF_subtrain)\n",
    "ss = StandardScaler()\n",
    "scaled_subtrain_TFIDF_NMF = ss.fit_transform(X_subtrain_TFIDF_NMF_normalized)\n",
    "\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "valid_corpus = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_NMF_valid = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('NMF', NMF(n_components=5,random_state=10))])\n",
    "TFIDF_NMF_valid = Pipeline_TFIDF_NMF_valid.fit_transform(valid_corpus)\n",
    "\n",
    "X_valid_TFIDF_NMF_normalized = normalize(TFIDF_NMF_valid)\n",
    "scaled_valid_TFIDF_NMF = ss.transform(X_valid_TFIDF_NMF_normalized)\n",
    "\n",
    "logreg_classwt_NMF = LogisticRegression(C=0.01, penalty='l2', solver='liblinear', class_weight='balanced', random_state=41, max_iter=500).fit(scaled_subtrain_TFIDF_NMF, y_subtrain)\n",
    "y_pred_classwt_NMF_subtrain = logreg_classwt_NMF.predict(scaled_subtrain_TFIDF_NMF)\n",
    "y_pred_classwt_NMF_valid = logreg_classwt_NMF.predict(scaled_valid_TFIDF_NMF)\n",
    "\n",
    "print(roc_auc_score(y_subtrain, y_pred_classwt_NMF_subtrain))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_NMF_valid))\n",
    "logreg_classwt_NMF.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T04:29:19.095375Z",
     "start_time": "2019-11-14T04:28:27.569312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.095*\"not\" + 0.081*\"like\" + 0.061*\"awesome\" + 0.048*\"feel\" + 0.044*\"animal\" + 0.043*\"original\" + 0.040*\"little\" + 0.039*\"real\" + 0.039*\"voice\" + 0.038*\"make\" + 0.038*\"character\" + 0.036*\"good\" + 0.036*\"look\" + 0.032*\"no\" + 0.032*\"cgi\" + 0.028*\"only\" + 0.023*\"way\" + 0.022*\"actor\" + 0.021*\"story\" + 0.021*\"scene\" + 0.020*\"act\" + 0.020*\"well\" + 0.018*\"song\" + 0.016*\"watch\" + 0.015*\"think\" + 0.014*\"great\" + 0.014*\"go\" + 0.014*\"time\" + 0.013*\"amaze\" + 0.011*\"cast\"'),\n",
       " (1,\n",
       "  '0.082*\"well\" + 0.069*\"disney\" + 0.055*\"live\" + 0.053*\"action\" + 0.052*\"not\" + 0.048*\"time\" + 0.042*\"live action\" + 0.042*\"lion\" + 0.040*\"king\" + 0.040*\"original\" + 0.038*\"go\" + 0.037*\"remake\" + 0.034*\"think\" + 0.033*\"watch\" + 0.025*\"good\" + 0.025*\"version\" + 0.024*\"make\" + 0.021*\"animate\" + 0.019*\"new\" + 0.018*\"classic\" + 0.017*\"love\" + 0.016*\"story\" + 0.016*\"like\" + 0.014*\"way\" + 0.012*\"scene\" + 0.012*\"character\" + 0.011*\"cast\" + 0.011*\"song\" + 0.010*\"great\" + 0.009*\"feel\"'),\n",
       " (2,\n",
       "  '0.151*\"great\" + 0.094*\"enjoy\" + 0.073*\"kid\" + 0.067*\"new\" + 0.060*\"classic\" + 0.057*\"version\" + 0.053*\"story\" + 0.052*\"old\" + 0.049*\"remake\" + 0.049*\"animate\" + 0.047*\"love\" + 0.045*\"original\" + 0.042*\"song\" + 0.036*\"not\" + 0.033*\"good\" + 0.027*\"disney\" + 0.022*\"watch\" + 0.020*\"like\" + 0.014*\"cast\" + 0.008*\"make\" + 0.000*\"time\" + 0.000*\"well\" + 0.000*\"character\" + 0.000*\"way\" + 0.000*\"think\" + 0.000*\"little\" + 0.000*\"go\" + 0.000*\"music\" + 0.000*\"actor\" + 0.000*\"fun\"'),\n",
       " (3,\n",
       "  '0.217*\"loved\" + 0.164*\"good\" + 0.140*\"beautiful\" + 0.139*\"music\" + 0.130*\"original\" + 0.043*\"great\" + 0.041*\"act\" + 0.039*\"not\" + 0.034*\"story\" + 0.026*\"scene\" + 0.012*\"watch\" + 0.009*\"cast\" + 0.005*\"make\" + 0.001*\"song\" + 0.000*\"like\" + 0.000*\"feel\" + 0.000*\"character\" + 0.000*\"actor\" + 0.000*\"love\" + 0.000*\"amaze\" + 0.000*\"new\" + 0.000*\"go\" + 0.000*\"disney\" + 0.000*\"think\" + 0.000*\"well\" + 0.000*\"time\" + 0.000*\"fun\" + 0.000*\"remake\" + 0.000*\"way\" + 0.000*\"awesome\"'),\n",
       " (4,\n",
       "  '0.195*\"love\" + 0.151*\"smith\" + 0.104*\"amaze\" + 0.092*\"genie\" + 0.074*\"great\" + 0.068*\"job\" + 0.067*\"fun\" + 0.059*\"aladdin\" + 0.027*\"good\" + 0.023*\"not\" + 0.021*\"cast\" + 0.019*\"make\" + 0.019*\"actor\" + 0.017*\"go\" + 0.017*\"watch\" + 0.015*\"think\" + 0.014*\"character\" + 0.010*\"song\" + 0.005*\"original\" + 0.004*\"disney\" + 0.000*\"music\" + 0.000*\"new\" + 0.000*\"well\" + 0.000*\"like\" + 0.000*\"story\" + 0.000*\"awesome\" + 0.000*\"way\" + 0.000*\"enjoy\" + 0.000*\"time\" + 0.000*\"act\"')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_df.drop(['rating','review_text','movie','review_site','review_text','sentiment', 'review_tokens'], axis=1), full_df['sentiment'], test_size=0.2, random_state=41)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "###SUBTRAIN\n",
    "corpus = X_train.iloc[:,0].tolist()\n",
    "\n",
    "tfidfvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec = tfidfvec.fit_transform(corpus)\n",
    "\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus = matutils.Sparse2Corpus(doc_word_tfidfvec.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=gensim_corpus, num_topics=5, id2word=id2word, passes=5)\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[gensim_corpus]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]\n",
    "\n",
    "tuples_doc_topic_df = pd.DataFrame(lda_docs,columns=['topic1','topic2','topic3','topic4','topic5'])\n",
    "tuples_doc_topic_df['topic1'] = tuples_doc_topic_df['topic1'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic2'] = tuples_doc_topic_df['topic2'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic3'] = tuples_doc_topic_df['topic3'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic4'] = tuples_doc_topic_df['topic4'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic5'] = tuples_doc_topic_df['topic5'].str[1].astype(float)\n",
    "\n",
    "X_train_TFIDF_LDA_normalized = normalize(tuples_doc_topic_df)\n",
    "ss = StandardScaler()\n",
    "scaled_train_TFIDF_LDA = ss.fit_transform(X_train_TFIDF_LDA_normalized)\n",
    "\n",
    "lda.print_topics(num_words=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T04:31:34.743466Z",
     "start_time": "2019-11-14T04:29:19.096914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:15<00:00,  3.69it/s, best loss: 0.27464862634403475]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.27464862634403475,\n",
       " 'params': {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, class_weight='balanced', random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_train_TFIDF_LDA, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T05:02:11.265180Z",
     "start_time": "2019-11-14T05:01:21.186151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6690377825461605\n",
      "0.659005837912088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.35001732, -0.14803274,  0.18460698,  0.18980734,  0.49158142]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "###SUBTRAIN\n",
    "corpus_subtrain = X_subtrain.iloc[:,0].tolist()\n",
    "\n",
    "tfidfvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec_subtrain = tfidfvec.fit_transform(corpus_subtrain)\n",
    "\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus_subtrain = matutils.Sparse2Corpus(doc_word_tfidfvec_subtrain.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda_subtrain = models.LdaModel(corpus=gensim_corpus_subtrain, num_topics=5, id2word=id2word, passes=5)\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus_subtrain = lda_subtrain[gensim_corpus_subtrain]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs_subtrain = [doc for doc in lda_corpus_subtrain]\n",
    "\n",
    "tuples_doc_topic_df = pd.DataFrame(lda_docs_subtrain,columns=['topic1','topic2','topic3','topic4','topic5'])\n",
    "tuples_doc_topic_df['topic1'] = tuples_doc_topic_df['topic1'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic2'] = tuples_doc_topic_df['topic2'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic3'] = tuples_doc_topic_df['topic3'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic4'] = tuples_doc_topic_df['topic4'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic5'] = tuples_doc_topic_df['topic5'].str[1].astype(float)\n",
    "\n",
    "X_subtrain_TFIDF_LDA_normalized = normalize(tuples_doc_topic_df)\n",
    "ss = StandardScaler()\n",
    "scaled_subtrain_TFIDF_LDA = ss.fit_transform(X_subtrain_TFIDF_LDA_normalized)\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "corpus_valid = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "\n",
    "tfidfvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec_valid = tfidfvec.fit_transform(corpus_valid)\n",
    "\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus_valid = matutils.Sparse2Corpus(doc_word_tfidfvec_valid.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda_valid = models.LdaModel(corpus=gensim_corpus_valid, num_topics=5, id2word=id2word, passes=5)\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus_valid = lda_valid[gensim_corpus_valid]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs_valid = [doc for doc in lda_corpus_valid]\n",
    "\n",
    "tuples_doc_topic_df = pd.DataFrame(lda_docs_valid,columns=['topic1','topic2','topic3','topic4','topic5'])\n",
    "tuples_doc_topic_df['topic1'] = tuples_doc_topic_df['topic1'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic2'] = tuples_doc_topic_df['topic2'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic3'] = tuples_doc_topic_df['topic3'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic4'] = tuples_doc_topic_df['topic4'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic5'] = tuples_doc_topic_df['topic5'].str[1].astype(float)\n",
    "\n",
    "X_valid_TFIDF_LDA_normalized = normalize(tuples_doc_topic_df)\n",
    "scaled_valid_TFIDF_LDA = ss.transform(X_valid_TFIDF_LDA_normalized)\n",
    "\n",
    "\n",
    "logreg_classwt_LDA = LogisticRegression(C=0.001, penalty='l2', solver='liblinear', class_weight='balanced', random_state=41, max_iter=500).fit(scaled_subtrain_TFIDF_LDA, y_subtrain)\n",
    "y_pred_classwt_LDA_subtrain = logreg_classwt_LDA.predict(scaled_subtrain_TFIDF_LDA)\n",
    "y_pred_classwt_LDA_valid = logreg_classwt_LDA.predict(scaled_valid_TFIDF_LDA)\n",
    "\n",
    "print(roc_auc_score(y_subtrain, y_pred_classwt_LDA_subtrain))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_LDA_valid))\n",
    "logreg_classwt_LDA.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T04:40:17.432147Z",
     "start_time": "2019-11-14T04:40:04.668176Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_df.drop(['rating','review_text','movie','review_site','review_text','sentiment', 'review_tokens'], axis=1), full_df['sentiment'], test_size=0.2, random_state=41)\n",
    "y_train = y_train.astype(int)\n",
    "corpus_NMF_tuning = X_train.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_NMF_tuning = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('NMF', NMF(n_components=14,random_state=10))])\n",
    "TFIDF_NMF_tuning = Pipeline_TFIDF_NMF_tuning.fit_transform(corpus_NMF_tuning)\n",
    "\n",
    "#Need to normalize before classifying to predict positive vs. negative sentiment\n",
    "X_train_TFIDF_NMF_normalized_tuning = normalize(TFIDF_NMF_tuning)\n",
    "\n",
    "\n",
    "X_train_TFIDF_NMF_tuning = pd.DataFrame(X_train_TFIDF_NMF_normalized_tuning,columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10','topic11','topic12','topic13','topic14'])\n",
    "#,'topic15','topic16','topic17','topic18','topic19','topic20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T04:44:41.044487Z",
     "start_time": "2019-11-14T04:41:26.158103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [03:14<00:00,  2.57it/s, best loss: 0.1873046887026517]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.1873046887026517,\n",
       " 'params': {'C': 0.05, 'penalty': 'l1', 'solver': 'liblinear'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train_TFIDF_NMF_tuning = StandardScaler().fit_transform(X_train_TFIDF_NMF_tuning)\n",
    "\n",
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, class_weight='balanced', random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_X_train_TFIDF_NMF_tuning, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T05:00:35.600739Z",
     "start_time": "2019-11-14T05:00:22.192848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7335070251879716\n",
      "0.7386733058608058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.58955892,  0.02189319,  0.69993711,  0.48412397,  0.20003813,\n",
       "         0.74054519, -0.26909712, -0.05055338,  0.08353921,  0.33712804,\n",
       "        -0.16208316, -0.11364424, -0.19890322,  0.35904309]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "###SUBTRAIN\n",
    "subtrain_corpus = X_subtrain.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_NMF_subtrain_tuning = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('NMF', NMF(n_components=14,random_state=10))])\n",
    "TFIDF_NMF_subtrain_tuning = Pipeline_TFIDF_NMF_subtrain_tuning.fit_transform(subtrain_corpus)\n",
    "\n",
    "X_subtrain_TFIDF_NMF_normalized_tuning= normalize(TFIDF_NMF_subtrain_tuning)\n",
    "X_subtrain_TFIDF_NMF_tuning = pd.DataFrame(X_subtrain_TFIDF_NMF_normalized_tuning,columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10','topic11','topic12','topic13','topic14'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "scaled_subtrain_TFIDF_NMF_tuning = ss.fit_transform(X_subtrain_TFIDF_NMF_tuning)\n",
    "\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "valid_corpus = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_NMF_valid = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('NMF', NMF(n_components=14,random_state=10))])\n",
    "TFIDF_NMF_valid = Pipeline_TFIDF_NMF_valid.fit_transform(valid_corpus)\n",
    "\n",
    "X_valid_TFIDF_NMF_normalized = normalize(TFIDF_NMF_valid)\n",
    "scaled_valid_TFIDF_NMF_tuning = ss.transform(X_valid_TFIDF_NMF_normalized)\n",
    "\n",
    "logreg_classwt_NMF_tuning = LogisticRegression(C=0.05, penalty='l1', solver='liblinear', class_weight='balanced', random_state=41, max_iter=500).fit(scaled_subtrain_TFIDF_NMF_tuning, y_subtrain)\n",
    "y_pred_classwt_NMF_subtrain_tuning = logreg_classwt_NMF_tuning.predict(scaled_subtrain_TFIDF_NMF_tuning)\n",
    "y_pred_classwt_NMF_valid_tuning = logreg_classwt_NMF_tuning.predict(scaled_valid_TFIDF_NMF_tuning)\n",
    "\n",
    "print(roc_auc_score(y_subtrain, y_pred_classwt_NMF_subtrain_tuning))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_NMF_valid_tuning))\n",
    "logreg_classwt_NMF_tuning.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T12:19:02.703770Z",
     "start_time": "2019-11-14T19:59:39.991Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T05:31:41.779220Z",
     "start_time": "2019-11-14T05:31:30.067372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7335070251879716\n",
      "0.7386733058608058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.58955892,  0.02189319,  0.69993711,  0.48412397,  0.20003813,\n",
       "         0.74054519, -0.26909712, -0.05055338,  0.08353921,  0.33712804,\n",
       "        -0.16208316, -0.11364424, -0.19890322,  0.35904309]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "###SUBTRAIN\n",
    "subtrain_corpus = X_subtrain.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "\n",
    "TFIDF_vec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05)\n",
    "TFIDF_subtrain_tuning = TFIDF_vec.fit_transform(subtrain_corpus)\n",
    "NMF_model = NMF(n_components=14,random_state=10)\n",
    "TFIDF_NMF_subtrain_tuning = NMF_model.fit_transform(TFIDF_subtrain_tuning)\n",
    "\n",
    "X_subtrain_TFIDF_NMF_normalized_tuning= normalize(TFIDF_NMF_subtrain_tuning)\n",
    "X_subtrain_TFIDF_NMF_tuning = pd.DataFrame(X_subtrain_TFIDF_NMF_normalized_tuning,columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10','topic11','topic12','topic13','topic14'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "scaled_subtrain_TFIDF_NMF_tuning = ss.fit_transform(X_subtrain_TFIDF_NMF_tuning)\n",
    "\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "valid_corpus = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "TFIDF_valid = TfidfVectorizer(ngram_range=(1,5), min_df=0.05).fit_transform(valid_corpus)\n",
    "TFIDF_NMF_valid = NMF(n_components=14,random_state=10).fit_transform(TFIDF_valid)\n",
    "\n",
    "X_valid_TFIDF_NMF_normalized = normalize(TFIDF_NMF_valid)\n",
    "scaled_valid_TFIDF_NMF_tuning = ss.transform(X_valid_TFIDF_NMF_normalized)\n",
    "\n",
    "logreg_classwt_NMF_tuning = LogisticRegression(C=0.05, penalty='l1', solver='liblinear', class_weight='balanced', random_state=41, max_iter=500).fit(scaled_subtrain_TFIDF_NMF_tuning, y_subtrain)\n",
    "y_pred_classwt_NMF_subtrain_tuning = logreg_classwt_NMF_tuning.predict(scaled_subtrain_TFIDF_NMF_tuning)\n",
    "y_pred_classwt_NMF_valid_tuning = logreg_classwt_NMF_tuning.predict(scaled_valid_TFIDF_NMF_tuning)\n",
    "\n",
    "print(roc_auc_score(y_subtrain, y_pred_classwt_NMF_subtrain_tuning))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_NMF_valid_tuning))\n",
    "logreg_classwt_NMF_tuning.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T05:32:51.100265Z",
     "start_time": "2019-11-14T05:32:51.062849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-2a2154c7a6f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNMF_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFIDF_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-133-9bbb74a704ef>\u001b[0m in \u001b[0;36mdisplay_topics\u001b[0;34m(model, feature_names, no_top_words, topic_names)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTopic: '\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         print(\", \".join([feature_names[i]\n\u001b[0;32m----> 8\u001b[0;31m                         for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-133-9bbb74a704ef>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTopic: '\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         print(\", \".join([feature_names[i]\n\u001b[0;32m----> 8\u001b[0;31m                         for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "display_topics(display_topics(NMF_model, TFIDF_vec.get_feature_names, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T16:21:13.305919Z",
     "start_time": "2019-11-14T16:21:00.455689Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_df.drop(['rating','review_text','movie','review_site','review_text','sentiment', 'review_tokens'], axis=1), full_df['sentiment'], test_size=0.2, random_state=41)\n",
    "y_train = y_train.astype(int)\n",
    "corpus_NMF_tuning = X_train.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_NMF_tuning = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('NMF', NMF(n_components=16,random_state=10))])\n",
    "TFIDF_NMF_tuning = Pipeline_TFIDF_NMF_tuning.fit_transform(corpus_NMF_tuning)\n",
    "\n",
    "#Need to normalize before classifying to predict positive vs. negative sentiment\n",
    "X_train_TFIDF_NMF_normalized_tuning = normalize(TFIDF_NMF_tuning)\n",
    "\n",
    "\n",
    "X_train_TFIDF_NMF_tuning = pd.DataFrame(X_train_TFIDF_NMF_normalized_tuning,columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10','topic11','topic12','topic13','topic14','topic15','topic16'])\n",
    "#,'topic11','topic12','topic13','topic14','topic15','topic16','topic17','topic18','topic19','topic20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T16:25:07.914449Z",
     "start_time": "2019-11-14T16:21:22.495934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [03:45<00:00,  2.22it/s, best loss: 0.1825314055824817] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.1825314055824817,\n",
       " 'params': {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train_TFIDF_NMF_tuning = StandardScaler().fit_transform(X_train_TFIDF_NMF_tuning)\n",
    "\n",
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, class_weight='balanced', random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_X_train_TFIDF_NMF_tuning, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T16:19:56.079293Z",
     "start_time": "2019-11-14T16:19:44.447519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7318905158280349\n",
      "0.6892427884615384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.62583928,  0.04444993,  0.63111498,  0.43729134,  0.16498511,\n",
       "         0.64590941, -0.27438959, -0.2349948 ,  0.15317076,  0.3100709 ]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "###SUBTRAIN\n",
    "subtrain_corpus = X_subtrain.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "\n",
    "TFIDF_vec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05)\n",
    "TFIDF_subtrain_tuning = TFIDF_vec.fit_transform(subtrain_corpus)\n",
    "NMF_model = NMF(n_components=16,random_state=10)\n",
    "TFIDF_NMF_subtrain_tuning = NMF_model.fit_transform(TFIDF_subtrain_tuning)\n",
    "\n",
    "X_subtrain_TFIDF_NMF_normalized_tuning= normalize(TFIDF_NMF_subtrain_tuning)\n",
    "X_subtrain_TFIDF_NMF_tuning = pd.DataFrame(X_subtrain_TFIDF_NMF_normalized_tuning,columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10','topic11','topic12','topic13','topic14','topic15','topic16'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "scaled_subtrain_TFIDF_NMF_tuning = ss.fit_transform(X_subtrain_TFIDF_NMF_tuning)\n",
    "\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "valid_corpus = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "TFIDF_valid = TfidfVectorizer(ngram_range=(1,5), min_df=0.05).fit_transform(valid_corpus)\n",
    "TFIDF_NMF_valid = NMF(n_components=16,random_state=10).fit_transform(TFIDF_valid)\n",
    "\n",
    "X_valid_TFIDF_NMF_normalized = normalize(TFIDF_NMF_valid)\n",
    "scaled_valid_TFIDF_NMF_tuning = ss.transform(X_valid_TFIDF_NMF_normalized)\n",
    "\n",
    "logreg_classwt_NMF_tuning = LogisticRegression(C=0.01, penalty='l2', solver='liblinear', class_weight='balanced', random_state=41, max_iter=500).fit(scaled_subtrain_TFIDF_NMF_tuning, y_subtrain)\n",
    "y_pred_classwt_NMF_subtrain_tuning = logreg_classwt_NMF_tuning.predict(scaled_subtrain_TFIDF_NMF_tuning)\n",
    "y_pred_classwt_NMF_valid_tuning = logreg_classwt_NMF_tuning.predict(scaled_valid_TFIDF_NMF_tuning)\n",
    "\n",
    "print(roc_auc_score(y_subtrain, y_pred_classwt_NMF_subtrain_tuning))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_NMF_valid_tuning))\n",
    "logreg_classwt_NMF_tuning.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:31:20.147457Z",
     "start_time": "2019-11-13T21:30:53.370Z"
    }
   },
   "outputs": [],
   "source": [
    "C_range=[0.001,0.05, 0.01,0.1,0.5,1,5, 10,100,500,1000,5000, 10000]\n",
    "solvers=['newton-cg', 'lbfgs', 'sag', 'saga', 'liblinear']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:31:20.151973Z",
     "start_time": "2019-11-13T21:30:53.382Z"
    }
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T17:11:21.580663Z",
     "start_time": "2019-11-14T17:10:26.452246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.494*\"awesome\" + 0.391*\"well\" + 0.047*\"original\" + 0.043*\"love\" + 0.019*\"disney\" + 0.003*\"make\" + 0.001*\"not\" + 0.000*\"song\" + 0.000*\"smith\" + 0.000*\"character\" + 0.000*\"music\" + 0.000*\"job\" + 0.000*\"cast\" + 0.000*\"think\" + 0.000*\"remake\" + 0.000*\"great\" + 0.000*\"good\" + 0.000*\"loved\" + 0.000*\"watch\" + 0.000*\"story\" + 0.000*\"like\" + 0.000*\"voice\" + 0.000*\"new\" + 0.000*\"cgi\" + 0.000*\"time\" + 0.000*\"way\" + 0.000*\"kid\" + 0.000*\"go\" + 0.000*\"amaze\" + 0.000*\"animate\"'),\n",
       " (1,\n",
       "  '0.102*\"aladdin\" + 0.086*\"think\" + 0.066*\"actor\" + 0.060*\"not\" + 0.045*\"smith\" + 0.045*\"good\" + 0.038*\"only\" + 0.035*\"genie\" + 0.034*\"great\" + 0.031*\"love\" + 0.024*\"like\" + 0.024*\"original\" + 0.024*\"disney\" + 0.024*\"make\" + 0.023*\"well\" + 0.023*\"character\" + 0.022*\"song\" + 0.022*\"go\" + 0.019*\"job\" + 0.018*\"new\" + 0.017*\"watch\" + 0.016*\"story\" + 0.015*\"cast\" + 0.015*\"time\" + 0.014*\"enjoy\" + 0.014*\"feel\" + 0.011*\"remake\" + 0.011*\"no\" + 0.011*\"version\" + 0.010*\"music\"'),\n",
       " (2,\n",
       "  '0.210*\"not\" + 0.203*\"like\" + 0.153*\"original\" + 0.116*\"go\" + 0.096*\"feel\" + 0.048*\"make\" + 0.038*\"watch\" + 0.034*\"love\" + 0.015*\"disney\" + 0.014*\"think\" + 0.014*\"song\" + 0.012*\"time\" + 0.012*\"good\" + 0.012*\"character\" + 0.010*\"kid\" + 0.006*\"new\" + 0.004*\"well\" + 0.001*\"great\" + 0.000*\"story\" + 0.000*\"remake\" + 0.000*\"only\" + 0.000*\"voice\" + 0.000*\"cast\" + 0.000*\"look\" + 0.000*\"scene\" + 0.000*\"little\" + 0.000*\"no\" + 0.000*\"real\" + 0.000*\"amaze\" + 0.000*\"music\"'),\n",
       " (3,\n",
       "  '0.149*\"story\" + 0.138*\"version\" + 0.109*\"new\" + 0.105*\"animate\" + 0.090*\"song\" + 0.074*\"original\" + 0.058*\"love\" + 0.048*\"not\" + 0.047*\"good\" + 0.038*\"great\" + 0.027*\"like\" + 0.025*\"well\" + 0.021*\"make\" + 0.017*\"disney\" + 0.016*\"character\" + 0.011*\"cast\" + 0.008*\"watch\" + 0.005*\"no\" + 0.005*\"time\" + 0.004*\"only\" + 0.003*\"think\" + 0.001*\"go\" + 0.000*\"look\" + 0.000*\"music\" + 0.000*\"feel\" + 0.000*\"voice\" + 0.000*\"actor\" + 0.000*\"little\" + 0.000*\"classic\" + 0.000*\"live\"'),\n",
       " (4,\n",
       "  '0.430*\"good\" + 0.188*\"remake\" + 0.129*\"time\" + 0.088*\"original\" + 0.075*\"not\" + 0.045*\"disney\" + 0.023*\"watch\" + 0.021*\"make\" + 0.000*\"well\" + 0.000*\"great\" + 0.000*\"love\" + 0.000*\"song\" + 0.000*\"story\" + 0.000*\"think\" + 0.000*\"kid\" + 0.000*\"job\" + 0.000*\"cast\" + 0.000*\"go\" + 0.000*\"like\" + 0.000*\"music\" + 0.000*\"animate\" + 0.000*\"new\" + 0.000*\"smith\" + 0.000*\"little\" + 0.000*\"amaze\" + 0.000*\"feel\" + 0.000*\"king\" + 0.000*\"lion\" + 0.000*\"way\" + 0.000*\"character\"'),\n",
       " (5,\n",
       "  '0.366*\"enjoy\" + 0.265*\"fun\" + 0.202*\"watch\" + 0.066*\"great\" + 0.056*\"kid\" + 0.024*\"original\" + 0.021*\"not\" + 0.000*\"good\" + 0.000*\"song\" + 0.000*\"make\" + 0.000*\"new\" + 0.000*\"think\" + 0.000*\"disney\" + 0.000*\"character\" + 0.000*\"animate\" + 0.000*\"feel\" + 0.000*\"love\" + 0.000*\"like\" + 0.000*\"music\" + 0.000*\"cast\" + 0.000*\"story\" + 0.000*\"go\" + 0.000*\"version\" + 0.000*\"time\" + 0.000*\"well\" + 0.000*\"voice\" + 0.000*\"smith\" + 0.000*\"loved\" + 0.000*\"little\" + 0.000*\"actor\"'),\n",
       " (6,\n",
       "  '0.096*\"animal\" + 0.086*\"real\" + 0.081*\"old\" + 0.070*\"look\" + 0.060*\"not\" + 0.054*\"no\" + 0.037*\"like\" + 0.029*\"love\" + 0.029*\"original\" + 0.027*\"make\" + 0.025*\"good\" + 0.024*\"lion\" + 0.022*\"king\" + 0.021*\"story\" + 0.021*\"great\" + 0.021*\"watch\" + 0.018*\"disney\" + 0.018*\"time\" + 0.017*\"character\" + 0.016*\"go\" + 0.016*\"feel\" + 0.016*\"think\" + 0.014*\"remake\" + 0.014*\"cgi\" + 0.014*\"well\" + 0.014*\"new\" + 0.012*\"little\" + 0.011*\"only\" + 0.011*\"song\" + 0.010*\"amaze\"'),\n",
       " (7,\n",
       "  '0.137*\"action\" + 0.136*\"live\" + 0.113*\"live action\" + 0.084*\"disney\" + 0.036*\"not\" + 0.035*\"remake\" + 0.028*\"original\" + 0.023*\"well\" + 0.023*\"animate\" + 0.023*\"version\" + 0.022*\"classic\" + 0.022*\"good\" + 0.021*\"make\" + 0.018*\"great\" + 0.015*\"cast\" + 0.015*\"story\" + 0.013*\"new\" + 0.013*\"love\" + 0.012*\"like\" + 0.012*\"time\" + 0.011*\"character\" + 0.011*\"song\" + 0.009*\"only\" + 0.009*\"watch\" + 0.009*\"think\" + 0.009*\"feel\" + 0.008*\"aladdin\" + 0.008*\"cgi\" + 0.008*\"go\" + 0.007*\"amaze\"'),\n",
       " (8,\n",
       "  '0.075*\"little\" + 0.075*\"act\" + 0.074*\"voice\" + 0.066*\"not\" + 0.057*\"way\" + 0.040*\"good\" + 0.038*\"character\" + 0.037*\"original\" + 0.028*\"well\" + 0.027*\"story\" + 0.027*\"like\" + 0.027*\"make\" + 0.025*\"great\" + 0.024*\"feel\" + 0.021*\"time\" + 0.021*\"actor\" + 0.020*\"disney\" + 0.017*\"cgi\" + 0.017*\"only\" + 0.017*\"song\" + 0.016*\"love\" + 0.016*\"watch\" + 0.016*\"cast\" + 0.016*\"go\" + 0.015*\"no\" + 0.014*\"think\" + 0.014*\"scene\" + 0.012*\"remake\" + 0.012*\"new\" + 0.011*\"animate\"'),\n",
       " (9,\n",
       "  '0.531*\"great\" + 0.241*\"music\" + 0.098*\"cast\" + 0.056*\"job\" + 0.034*\"original\" + 0.019*\"disney\" + 0.013*\"character\" + 0.004*\"song\" + 0.002*\"make\" + 0.001*\"love\" + 0.001*\"story\" + 0.000*\"smith\" + 0.000*\"new\" + 0.000*\"not\" + 0.000*\"remake\" + 0.000*\"loved\" + 0.000*\"good\" + 0.000*\"enjoy\" + 0.000*\"genie\" + 0.000*\"classic\" + 0.000*\"time\" + 0.000*\"go\" + 0.000*\"act\" + 0.000*\"feel\" + 0.000*\"aladdin\" + 0.000*\"fun\" + 0.000*\"think\" + 0.000*\"amaze\" + 0.000*\"watch\" + 0.000*\"well\"'),\n",
       " (10,\n",
       "  '0.491*\"amaze\" + 0.365*\"beautiful\" + 0.065*\"love\" + 0.027*\"story\" + 0.017*\"disney\" + 0.016*\"original\" + 0.006*\"watch\" + 0.006*\"make\" + 0.002*\"song\" + 0.001*\"cast\" + 0.000*\"not\" + 0.000*\"music\" + 0.000*\"character\" + 0.000*\"real\" + 0.000*\"look\" + 0.000*\"cgi\" + 0.000*\"great\" + 0.000*\"feel\" + 0.000*\"good\" + 0.000*\"time\" + 0.000*\"animal\" + 0.000*\"job\" + 0.000*\"like\" + 0.000*\"new\" + 0.000*\"well\" + 0.000*\"scene\" + 0.000*\"actor\" + 0.000*\"loved\" + 0.000*\"animate\" + 0.000*\"think\"'),\n",
       " (11,\n",
       "  '0.295*\"smith\" + 0.171*\"genie\" + 0.099*\"kid\" + 0.089*\"great\" + 0.082*\"job\" + 0.063*\"love\" + 0.043*\"good\" + 0.029*\"make\" + 0.026*\"loved\" + 0.022*\"not\" + 0.016*\"amaze\" + 0.015*\"character\" + 0.009*\"aladdin\" + 0.009*\"song\" + 0.008*\"cast\" + 0.008*\"original\" + 0.004*\"disney\" + 0.004*\"think\" + 0.003*\"watch\" + 0.003*\"go\" + 0.001*\"new\" + 0.000*\"little\" + 0.000*\"like\" + 0.000*\"story\" + 0.000*\"time\" + 0.000*\"well\" + 0.000*\"feel\" + 0.000*\"enjoy\" + 0.000*\"remake\" + 0.000*\"music\"'),\n",
       " (12,\n",
       "  '0.169*\"classic\" + 0.110*\"scene\" + 0.076*\"cgi\" + 0.053*\"disney\" + 0.047*\"not\" + 0.043*\"original\" + 0.040*\"remake\" + 0.032*\"great\" + 0.031*\"good\" + 0.030*\"new\" + 0.029*\"story\" + 0.025*\"make\" + 0.022*\"character\" + 0.022*\"animate\" + 0.020*\"song\" + 0.020*\"well\" + 0.018*\"watch\" + 0.017*\"cast\" + 0.015*\"only\" + 0.015*\"feel\" + 0.015*\"time\" + 0.014*\"like\" + 0.013*\"love\" + 0.012*\"think\" + 0.011*\"go\" + 0.009*\"way\" + 0.008*\"enjoy\" + 0.008*\"no\" + 0.007*\"music\" + 0.007*\"job\"'),\n",
       " (13,\n",
       "  '0.392*\"love\" + 0.360*\"loved\" + 0.100*\"lion\" + 0.096*\"king\" + 0.038*\"original\" + 0.007*\"make\" + 0.003*\"not\" + 0.002*\"song\" + 0.000*\"character\" + 0.000*\"new\" + 0.000*\"watch\" + 0.000*\"disney\" + 0.000*\"remake\" + 0.000*\"music\" + 0.000*\"amaze\" + 0.000*\"great\" + 0.000*\"story\" + 0.000*\"well\" + 0.000*\"go\" + 0.000*\"like\" + 0.000*\"feel\" + 0.000*\"time\" + 0.000*\"kid\" + 0.000*\"good\" + 0.000*\"version\" + 0.000*\"way\" + 0.000*\"cast\" + 0.000*\"think\" + 0.000*\"only\" + 0.000*\"enjoy\"')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(full_df.drop(['rating','review_text','movie','review_site','review_text','sentiment', 'review_tokens'], axis=1), full_df['sentiment'], test_size=0.2, random_state=41)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "###SUBTRAIN\n",
    "corpus = X_train.iloc[:,0].tolist()\n",
    "\n",
    "tfidfvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec = tfidfvec.fit_transform(corpus)\n",
    "\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus = matutils.Sparse2Corpus(doc_word_tfidfvec.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=gensim_corpus, num_topics=14, id2word=id2word, passes=5)\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[gensim_corpus]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]\n",
    "\n",
    "tuples_doc_topic_df = pd.DataFrame(lda_docs,columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10','topic11','topic12','topic13','topic14'])\n",
    "tuples_doc_topic_df['topic1'] = tuples_doc_topic_df['topic1'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic2'] = tuples_doc_topic_df['topic2'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic3'] = tuples_doc_topic_df['topic3'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic4'] = tuples_doc_topic_df['topic4'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic5'] = tuples_doc_topic_df['topic5'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic6'] = tuples_doc_topic_df['topic6'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic7'] = tuples_doc_topic_df['topic7'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic8'] = tuples_doc_topic_df['topic8'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic9'] = tuples_doc_topic_df['topic9'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic10'] = tuples_doc_topic_df['topic10'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic11'] = tuples_doc_topic_df['topic11'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic12'] = tuples_doc_topic_df['topic12'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic13'] = tuples_doc_topic_df['topic13'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic14'] = tuples_doc_topic_df['topic14'].str[1].astype(float)\n",
    "\n",
    "\n",
    "X_train_TFIDF_LDA_normalized = normalize(tuples_doc_topic_df)\n",
    "ss = StandardScaler()\n",
    "scaled_train_TFIDF_LDA = ss.fit_transform(X_train_TFIDF_LDA_normalized)\n",
    "\n",
    "lda.print_topics(num_words=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T17:17:30.030188Z",
     "start_time": "2019-11-14T17:11:37.460115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [05:52<00:00,  1.42it/s, best loss: 0.2658697422484009]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.2658697422484009,\n",
       " 'params': {'C': 0.5, 'penalty': 'l2', 'solver': 'saga'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, class_weight='balanced', random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_train_TFIDF_LDA, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T18:14:27.283961Z",
     "start_time": "2019-11-14T18:13:40.764344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6719267987978159\n",
      "0.6613266941391941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.02953824, -0.18467647, -0.38152271, -0.15113533, -0.132297  ,\n",
       "         0.22309397, -0.34444772, -0.14276446, -0.46266649,  0.2934862 ,\n",
       "         0.34885669,  0.23763806, -0.2431188 ,  0.4149495 ]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "###SUBTRAIN\n",
    "corpus_subtrain = X_subtrain.iloc[:,0].tolist()\n",
    "\n",
    "tfidfvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec_subtrain = tfidfvec.fit_transform(corpus_subtrain)\n",
    "\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus_subtrain = matutils.Sparse2Corpus(doc_word_tfidfvec_subtrain.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda_subtrain = models.LdaModel(corpus=gensim_corpus_subtrain, num_topics=14, id2word=id2word, passes=5)\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus_subtrain = lda_subtrain[gensim_corpus_subtrain]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs_subtrain = [doc for doc in lda_corpus_subtrain]\n",
    "\n",
    "tuples_doc_topic_df = pd.DataFrame(lda_docs_subtrain,columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10','topic11','topic12','topic13','topic14'])\n",
    "tuples_doc_topic_df['topic1'] = tuples_doc_topic_df['topic1'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic2'] = tuples_doc_topic_df['topic2'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic3'] = tuples_doc_topic_df['topic3'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic4'] = tuples_doc_topic_df['topic4'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic5'] = tuples_doc_topic_df['topic5'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic6'] = tuples_doc_topic_df['topic6'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic7'] = tuples_doc_topic_df['topic7'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic8'] = tuples_doc_topic_df['topic8'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic9'] = tuples_doc_topic_df['topic9'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic10'] = tuples_doc_topic_df['topic10'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic11'] = tuples_doc_topic_df['topic11'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic12'] = tuples_doc_topic_df['topic12'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic13'] = tuples_doc_topic_df['topic13'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic14'] = tuples_doc_topic_df['topic14'].str[1].astype(float)\n",
    "\n",
    "X_subtrain_TFIDF_LDA_normalized = normalize(tuples_doc_topic_df)\n",
    "ss = StandardScaler()\n",
    "scaled_subtrain_TFIDF_LDA = ss.fit_transform(X_subtrain_TFIDF_LDA_normalized)\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "corpus_valid = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "\n",
    "tfidfvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec_valid = tfidfvec.fit_transform(corpus_valid)\n",
    "\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus_valid = matutils.Sparse2Corpus(doc_word_tfidfvec_valid.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda_valid = models.LdaModel(corpus=gensim_corpus_valid, num_topics=14, id2word=id2word, passes=5)\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus_valid = lda_valid[gensim_corpus_valid]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs_valid = [doc for doc in lda_corpus_valid]\n",
    "\n",
    "tuples_doc_topic_df = pd.DataFrame(lda_docs_valid,columns=['topic1','topic2','topic3','topic4','topic5','topic6','topic7','topic8','topic9','topic10','topic11','topic12','topic13','topic14'])\n",
    "tuples_doc_topic_df['topic1'] = tuples_doc_topic_df['topic1'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic2'] = tuples_doc_topic_df['topic2'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic3'] = tuples_doc_topic_df['topic3'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic4'] = tuples_doc_topic_df['topic4'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic5'] = tuples_doc_topic_df['topic5'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic6'] = tuples_doc_topic_df['topic6'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic7'] = tuples_doc_topic_df['topic7'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic8'] = tuples_doc_topic_df['topic8'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic9'] = tuples_doc_topic_df['topic9'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic10'] = tuples_doc_topic_df['topic10'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic11'] = tuples_doc_topic_df['topic11'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic12'] = tuples_doc_topic_df['topic12'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic13'] = tuples_doc_topic_df['topic13'].str[1].astype(float)\n",
    "tuples_doc_topic_df['topic14'] = tuples_doc_topic_df['topic14'].str[1].astype(float)\n",
    "\n",
    "X_valid_TFIDF_LDA_normalized = normalize(tuples_doc_topic_df)\n",
    "scaled_valid_TFIDF_LDA = ss.transform(X_valid_TFIDF_LDA_normalized)\n",
    "\n",
    "\n",
    "logreg_classwt_LDA = LogisticRegression(C=0.5, penalty='l2', solver='saga', class_weight='balanced', random_state=41, max_iter=500).fit(scaled_subtrain_TFIDF_LDA, y_subtrain)\n",
    "y_pred_classwt_LDA_subtrain = logreg_classwt_LDA.predict(scaled_subtrain_TFIDF_LDA)\n",
    "y_pred_classwt_LDA_valid = logreg_classwt_LDA.predict(scaled_valid_TFIDF_LDA)\n",
    "\n",
    "print(roc_auc_score(y_subtrain, y_pred_classwt_LDA_subtrain))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_LDA_valid))\n",
    "logreg_classwt_LDA.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:31:20.153659Z",
     "start_time": "2019-11-13T21:30:53.385Z"
    }
   },
   "source": [
    "# Random Oversampling - LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:48:57.525077Z",
     "start_time": "2019-11-13T23:48:45.526348Z"
    }
   },
   "outputs": [],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "# #RANDOM Oversampling\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "X_ros_resampled_subtrain, y_ros_resampled_subtrain = ros.fit_sample(X_subtrain,y_subtrain)\n",
    "ros_corpus = pd.DataFrame(X_ros_resampled_subtrain).iloc[:,0].tolist() #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_LSA_ros = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('LSA', TruncatedSVD(n_components=5,random_state=10, n_iter=10))])\n",
    "TFIDF_LSA_ros = Pipeline_TFIDF_LSA_ros.fit_transform(ros_corpus)\n",
    "#Need to normalize before classifying to predict positive vs. negative sentiment\n",
    "X_subtrain_TFIDF_LSA_ros_normalized = normalize(TFIDF_LSA_ros)\n",
    "\n",
    "X_TFIDF_LSA_ros_resampled_subtrain = pd.DataFrame(X_subtrain_TFIDF_LSA_ros_normalized, columns = ['topic1', 'topic2', 'topic3', 'topic4', 'topic5'])\n",
    "y_TFIDF_LSA_ros_resampled_subtrain = pd.DataFrame(y_ros_resampled_subtrain, columns = ['sentiment'])\n",
    "\n",
    "scaled_X_subtrain_TFIDF_LSA_ros = StandardScaler().fit_transform(X_TFIDF_LSA_ros_resampled_subtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:51:45.257878Z",
     "start_time": "2019-11-13T23:49:23.683039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [02:21<00:00,  3.53it/s, best loss: 0.21908547822784719]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.21908547822784719,\n",
       " 'params': {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_X_subtrain_TFIDF_LSA_ros, y_ros_resampled_subtrain, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T12:19:02.705148Z",
     "start_time": "2019-11-14T21:14:11.541Z"
    }
   },
   "outputs": [],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "\n",
    "# #SUBTRAIN\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "X_ros_resampled_subtrain, y_ros_resampled_subtrain = ros.fit_sample(X_subtrain,y_subtrain)\n",
    "ros_corpus = pd.DataFrame(X_ros_resampled_subtrain).iloc[:,0].tolist() #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_LSA_ros = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('LSA', TruncatedSVD(n_components=5,random_state=10, n_iter=10))])\n",
    "TFIDF_LSA_ros = Pipeline_TFIDF_LSA_ros.fit_transform(ros_corpus)\n",
    "#Need to normalize before classifying to predict positive vs. negative sentiment\n",
    "X_subtrain_TFIDF_LSA_ros_normalized = normalize(TFIDF_LSA_ros)\n",
    "\n",
    "X_TFIDF_LSA_ros_resampled_subtrain = pd.DataFrame(X_subtrain_TFIDF_LSA_ros_normalized, columns = ['topic1', 'topic2', 'topic3', 'topic4', 'topic5'])\n",
    "y_TFIDF_LSA_ros_resampled_subtrain = pd.DataFrame(y_ros_resampled_subtrain, columns = ['sentiment'])\n",
    "\n",
    "ss= StandardScaler()\n",
    "scaled_X_subtrain_TFIDF_LSA_ros = ss.fit_transform(X_TFIDF_LSA_ros_resampled_subtrain)\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "valid_corpus = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_LSA_valid = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('LSA', TruncatedSVD(n_components=5,random_state=10))])\n",
    "TFIDF_LSA_valid = Pipeline_TFIDF_LSA_valid.fit_transform(valid_corpus)\n",
    "\n",
    "X_valid_TFIDF_LSA_normalized = normalize(TFIDF_LSA_valid)\n",
    "scaled_valid_TFIDF_LSA = ss.transform(X_valid_TFIDF_LSA_normalized)\n",
    "\n",
    "logreg_classwt_LSA = LogisticRegression(C=0.01, penalty='l2', solver='liblinear', random_state=41, max_iter=500).fit(scaled_X_subtrain_TFIDF_LSA_ros, y_ros_resampled_subtrain)\n",
    "y_pred_classwt_LSA_subtrain = logreg_classwt_LSA.predict(scaled_X_subtrain_TFIDF_LSA_ros)\n",
    "y_pred_classwt_LSA_valid = logreg_classwt_LSA.predict(scaled_valid_TFIDF_LSA)\n",
    "\n",
    "print(roc_auc_score(y_ros_resampled_subtrain, y_pred_classwt_LSA_subtrain))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_LSA_valid))\n",
    "logreg_classwt_LSA.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Oversampling - LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:53:17.453470Z",
     "start_time": "2019-11-13T23:52:59.081455Z"
    }
   },
   "outputs": [],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "tvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05)\n",
    "testing_tfidf = tvec.fit_transform(X_subtrain.iloc[:,0].tolist())\n",
    "\n",
    "#SMOTE Oversampling\n",
    "X_smoted_subtrain, y_smoted_subtrain = SMOTE(random_state=0).fit_sample(testing_tfidf,y_subtrain)\n",
    "TFIDF_LSA_smote = TruncatedSVD(n_components=5,random_state=10, n_iter=10).fit_transform(X_smoted_subtrain)\n",
    "X_subtrain_TFIDF_LSA_smote_normalized = normalize(TFIDF_LSA_smote)\n",
    "\n",
    "X_TFIDF_LSA_smoted_subtrain = pd.DataFrame(X_subtrain_TFIDF_LSA_smote_normalized, columns = ['topic1', 'topic2', 'topic3', 'topic4', 'topic5'])\n",
    "y_TFIDF_LSA_smoted_subtrain = pd.DataFrame(y_smoted_subtrain, columns = ['sentiment'])\n",
    "\n",
    "scaled_X_subtrain_TFIDF_LSA_smoted = StandardScaler().fit_transform(X_TFIDF_LSA_smoted_subtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T23:57:43.626419Z",
     "start_time": "2019-11-13T23:53:22.603431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [04:21<00:00,  1.92it/s, best loss: 0.18894382469842186]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.18894382469842186,\n",
       " 'params': {'C': 10, 'penalty': 'l2', 'solver': 'saga'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_X_subtrain_TFIDF_LSA_smoted, y_smoted_subtrain, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T03:25:53.967301Z",
     "start_time": "2019-11-14T03:25:33.756376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7394685677252105\n",
      "0.6276127518315019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.13040146,  1.02771128,  0.1743759 , -0.48544082,  0.6308184 ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "# #SUBTRAIN\n",
    "tvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05)\n",
    "testing_tfidf = tvec.fit_transform(X_subtrain.iloc[:,0].tolist())\n",
    "\n",
    "#SMOTE Oversampling\n",
    "X_smoted_subtrain, y_smoted_subtrain = SMOTE(random_state=0).fit_sample(testing_tfidf,y_subtrain)\n",
    "TFIDF_LSA_smote = TruncatedSVD(n_components=5,random_state=10, n_iter=10).fit_transform(X_smoted_subtrain)\n",
    "X_subtrain_TFIDF_LSA_smote_normalized = normalize(TFIDF_LSA_smote)\n",
    "\n",
    "X_TFIDF_LSA_smoted_subtrain = pd.DataFrame(X_subtrain_TFIDF_LSA_smote_normalized, columns = ['topic1', 'topic2', 'topic3', 'topic4', 'topic5'])\n",
    "y_TFIDF_LSA_smoted_subtrain = pd.DataFrame(y_smoted_subtrain, columns = ['sentiment'])\n",
    "\n",
    "ss= StandardScaler()\n",
    "scaled_X_subtrain_TFIDF_LSA_smoted = ss.fit_transform(X_TFIDF_LSA_smoted_subtrain)\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "valid_corpus = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_LSA_valid = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('LSA', TruncatedSVD(n_components=5,random_state=10))])\n",
    "TFIDF_LSA_valid = Pipeline_TFIDF_LSA_valid.fit_transform(valid_corpus)\n",
    "\n",
    "X_valid_TFIDF_LSA_normalized = normalize(TFIDF_LSA_valid)\n",
    "scaled_valid_TFIDF_LSA = ss.transform(X_valid_TFIDF_LSA_normalized)\n",
    "\n",
    "logreg_classwt_LSA = LogisticRegression(C=10, penalty='l2', solver='saga', random_state=41, max_iter=500).fit(scaled_X_subtrain_TFIDF_LSA_smoted, y_smoted_subtrain)\n",
    "y_pred_classwt_LSA_subtrain = logreg_classwt_LSA.predict(scaled_X_subtrain_TFIDF_LSA_smoted)\n",
    "y_pred_classwt_LSA_valid = logreg_classwt_LSA.predict(scaled_valid_TFIDF_LSA)\n",
    "\n",
    "print(roc_auc_score(y_smoted_subtrain, y_pred_classwt_LSA_subtrain))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_LSA_valid))\n",
    "logreg_classwt_LSA.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADASYN Oversampling - LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T00:22:45.615417Z",
     "start_time": "2019-11-14T00:22:26.942748Z"
    }
   },
   "outputs": [],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "tvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05)\n",
    "testing_tfidf = tvec.fit_transform(X_subtrain.iloc[:,0].tolist())\n",
    "\n",
    "# #ADASYN Oversampling\n",
    "X_adasyn_subtrain, y_adasyn_subtrain = ADASYN(random_state=0).fit_sample(testing_tfidf,y_subtrain)\n",
    "TFIDF_LSA_adasyn = TruncatedSVD(n_components=5,random_state=10, n_iter=10).fit_transform(X_adasyn_subtrain)\n",
    "X_subtrain_TFIDF_LSA_adasyn_normalized = normalize(TFIDF_LSA_adasyn)\n",
    "\n",
    "X_TFIDF_LSA_adasyn_subtrain = pd.DataFrame(X_subtrain_TFIDF_LSA_adasyn_normalized, columns = ['topic1', 'topic2', 'topic3', 'topic4', 'topic5'])\n",
    "y_TFIDF_LSA_adasyn_subtrain = pd.DataFrame(y_adasyn_subtrain, columns = ['sentiment'])\n",
    "\n",
    "scaled_X_subtrain_TFIDF_LSA_adasyn = StandardScaler().fit_transform(X_TFIDF_LSA_adasyn_subtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T00:27:23.740965Z",
     "start_time": "2019-11-14T00:25:28.041667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:55<00:00,  4.32it/s, best loss: 0.2174162322839449]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.2174162322839449,\n",
       " 'params': {'C': 5000, 'penalty': 'l2', 'solver': 'liblinear'},\n",
       " 'status': 'ok'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = {'C': hp.choice('C', [0.001,0.05, 0.01,0.1,0.5,1,5, 10,50,100,500,1000,5000, 10000]),\n",
    "        'solver': hp.choice('solver', ['saga', 'liblinear']),\n",
    "        'penalty': hp.choice('penalty', ['l1','l2'])}\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    \"\"\"Objective function for Logistic Regerssion Machine Hyperparameter Tuning\"\"\"\n",
    "    \n",
    "    # Perform n_fold cross validation with hyperparameters\n",
    "    # Use early stopping and evalute based on ROC AUC\n",
    "    \n",
    "    model = LogisticRegression(**params, random_state=41, verbose=0,max_iter=500)\n",
    "    \n",
    "    best_score = cross_val_score(model, scaled_X_subtrain_TFIDF_LSA_adasyn, y_adasyn_subtrain, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "MAX_EVALS = 500\n",
    "trials = Trials()\n",
    "# We initialize trials object here to be able to see our results after algorithm is complete\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = MAX_EVALS,\n",
    "            trials= trials)\n",
    "best\n",
    "\n",
    "# To see which results were best\n",
    "best_results = sorted(trials.results, key = lambda x: x['loss'])\n",
    "best_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T03:18:15.432549Z",
     "start_time": "2019-11-14T03:17:53.542890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7235260791672052\n",
      "0.6111921932234432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06847054,  0.95082449,  0.29788415, -0.56833238,  0.60619674]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subtrain, X_valid, y_subtrain, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=41)\n",
    "\n",
    "# #SUBTRAIN\n",
    "tvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05)\n",
    "testing_tfidf = tvec.fit_transform(X_subtrain.iloc[:,0].tolist())\n",
    "\n",
    "# #ADASYN Oversampling\n",
    "X_adasyn_subtrain, y_adasyn_subtrain = ADASYN(random_state=0).fit_sample(testing_tfidf,y_subtrain)\n",
    "TFIDF_LSA_adasyn = TruncatedSVD(n_components=5,random_state=10, n_iter=10).fit_transform(X_adasyn_subtrain)\n",
    "X_subtrain_TFIDF_LSA_adasyn_normalized = normalize(TFIDF_LSA_adasyn)\n",
    "\n",
    "X_TFIDF_LSA_adasyn_subtrain = pd.DataFrame(X_subtrain_TFIDF_LSA_adasyn_normalized, columns = ['topic1', 'topic2', 'topic3', 'topic4', 'topic5'])\n",
    "y_TFIDF_LSA_adasyn_subtrain = pd.DataFrame(y_adasyn_subtrain, columns = ['sentiment'])\n",
    "\n",
    "ss= StandardScaler()\n",
    "scaled_X_subtrain_TFIDF_LSA_adasyn = ss.fit_transform(X_TFIDF_LSA_adasyn_subtrain)\n",
    "\n",
    "\n",
    "###VALID\n",
    "\n",
    "valid_corpus = X_valid.iloc[:,0].tolist()  #convert from dataframe to series, to transform into list of reviews\n",
    "Pipeline_TFIDF_LSA_valid = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(ngram_range=(1,5), min_df=0.05)),\n",
    "                ('LSA', TruncatedSVD(n_components=5,random_state=10))])\n",
    "TFIDF_LSA_valid = Pipeline_TFIDF_LSA_valid.fit_transform(valid_corpus)\n",
    "\n",
    "X_valid_TFIDF_LSA_normalized = normalize(TFIDF_LSA_valid)\n",
    "scaled_valid_TFIDF_LSA = ss.transform(X_valid_TFIDF_LSA_normalized)\n",
    "\n",
    "logreg_classwt_LSA = LogisticRegression(C=5000, penalty='l2', solver='liblinear', random_state=41, max_iter=500).fit(scaled_X_subtrain_TFIDF_LSA_adasyn, y_adasyn_subtrain)\n",
    "y_pred_classwt_LSA_subtrain = logreg_classwt_LSA.predict(scaled_X_subtrain_TFIDF_LSA_adasyn)\n",
    "y_pred_classwt_LSA_valid = logreg_classwt_LSA.predict(scaled_valid_TFIDF_LSA)\n",
    "\n",
    "print(roc_auc_score(y_adasyn_subtrain, y_pred_classwt_LSA_subtrain))\n",
    "print(roc_auc_score(y_valid, y_pred_classwt_LSA_valid))\n",
    "logreg_classwt_LSA.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDP\n",
    "Gensim also provides a Hierarchical Dirichlet Process (HDP) class [5]. HDP is similar to LDA, except it seeks to learn the correct number of topics from the data; that is, you donâ€™t need to provide a fixed number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:18:32.034227Z",
     "start_time": "2019-11-14T19:18:32.030548Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = X_train.iloc[:,0].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:25:30.667121Z",
     "start_time": "2019-11-14T19:25:30.017926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'live',\n",
       " 'remake',\n",
       " 'cgi',\n",
       " 'well',\n",
       " 'cartoon',\n",
       " 'effect',\n",
       " 'stop',\n",
       " 'messing',\n",
       " 'classics',\n",
       " 'no',\n",
       " 'magic',\n",
       " 'no',\n",
       " 'humor',\n",
       " 'remake',\n",
       " 'never',\n",
       " 'happen',\n",
       " 'great',\n",
       " 'movie',\n",
       " 'time',\n",
       " 'not',\n",
       " 'remake',\n",
       " 'compare',\n",
       " 'terribly',\n",
       " 'disappoint',\n",
       " 'enough',\n",
       " 'enough',\n",
       " 'stop',\n",
       " 'remakes',\n",
       " 'my',\n",
       " 'star',\n",
       " 'cgi',\n",
       " 'special',\n",
       " 'effect',\n",
       " 'team',\n",
       " 'only',\n",
       " 'despite',\n",
       " 'fact',\n",
       " 'animal',\n",
       " 'talk',\n",
       " 'lose',\n",
       " 'film',\n",
       " 'mirror',\n",
       " 'animate',\n",
       " 'version',\n",
       " 'add',\n",
       " 'realism',\n",
       " 'enjoyable',\n",
       " 'doe',\n",
       " 'not',\n",
       " 'deserve',\n",
       " 'hate',\n",
       " 'beautiful',\n",
       " 'watch',\n",
       " 'great',\n",
       " 'cgi',\n",
       " 'cute',\n",
       " 'animal',\n",
       " 'funny',\n",
       " 'moment',\n",
       " 'good',\n",
       " 'voice',\n",
       " 'act',\n",
       " 'ask',\n",
       " 'for',\n",
       " 'i',\n",
       " 'think',\n",
       " 'write',\n",
       " 'scene',\n",
       " 'work',\n",
       " 'well',\n",
       " 'original',\n",
       " 'version',\n",
       " 'especially',\n",
       " 'scar',\n",
       " 'villain',\n",
       " 'song',\n",
       " 'hyena',\n",
       " 'scary',\n",
       " 'ted',\n",
       " 'weirdo',\n",
       " 'main',\n",
       " 'hyena',\n",
       " 'lot',\n",
       " 'personality',\n",
       " 'original',\n",
       " 'movie',\n",
       " 'wich',\n",
       " 'nice',\n",
       " 'favourite',\n",
       " 'og',\n",
       " 'movie',\n",
       " 'simba',\n",
       " 'timon',\n",
       " 'pumbaa',\n",
       " 'bachelor',\n",
       " 'jungle',\n",
       " 'amazing',\n",
       " 'new',\n",
       " 'movie',\n",
       " 'surprize',\n",
       " 'nala',\n",
       " 'appear',\n",
       " 'know',\n",
       " 'point',\n",
       " 'kinda',\n",
       " 'people',\n",
       " 'mean',\n",
       " 'complain',\n",
       " 'emotion',\n",
       " 'facial',\n",
       " 'ecpressions',\n",
       " 'think',\n",
       " 'person',\n",
       " 'compassionate',\n",
       " 'animal',\n",
       " 'lot',\n",
       " 'emotion',\n",
       " 'movie',\n",
       " 'animal',\n",
       " 'doesen',\n",
       " 't',\n",
       " 'human',\n",
       " 'eyebrow',\n",
       " 'feel',\n",
       " 'kinda',\n",
       " 'weird',\n",
       " 'ufeff',\n",
       " 'minute',\n",
       " 'use',\n",
       " 'face',\n",
       " 'bodylanguage',\n",
       " 'work',\n",
       " 'not',\n",
       " 'think',\n",
       " 'enjoyable',\n",
       " 'film',\n",
       " 'worth',\n",
       " 'watch',\n",
       " 'maybe',\n",
       " 'loved',\n",
       " 'favorite',\n",
       " 'cinderella',\n",
       " 'old',\n",
       " 'no',\n",
       " 'facial',\n",
       " 'expression',\n",
       " 'real',\n",
       " 'animal',\n",
       " 'nothing',\n",
       " 'different',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'smith',\n",
       " 'great',\n",
       " 'movie',\n",
       " 'fun',\n",
       " 'watch',\n",
       " 'movie',\n",
       " 'doe',\n",
       " 'suppose',\n",
       " 'add',\n",
       " 'original',\n",
       " 'make',\n",
       " 'young',\n",
       " 'audience',\n",
       " 'fall',\n",
       " 'love',\n",
       " 'story',\n",
       " 'flaw',\n",
       " 'song',\n",
       " 'tell',\n",
       " 'auto',\n",
       " 'tune',\n",
       " 'object',\n",
       " 'look',\n",
       " 'great',\n",
       " 'lumiere',\n",
       " 'spin',\n",
       " 'another',\n",
       " 'candle',\n",
       " 'not',\n",
       " 'make',\n",
       " 'sense',\n",
       " 'overall',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'great',\n",
       " 'movie',\n",
       " 'ufeff',\n",
       " 'min',\n",
       " 'spend',\n",
       " 'compare',\n",
       " 'head',\n",
       " 'cartoon',\n",
       " 'original',\n",
       " 'realize',\n",
       " 'make',\n",
       " 'no',\n",
       " 'sense',\n",
       " 'sit',\n",
       " 'enjoy',\n",
       " 'fun',\n",
       " 'enjoyable',\n",
       " 'movie',\n",
       " 'smith',\n",
       " 'funny',\n",
       " 'think',\n",
       " 'robin',\n",
       " 'williams',\n",
       " 'love',\n",
       " 'movie',\n",
       " 'amazing',\n",
       " 'seriously',\n",
       " 'well',\n",
       " 'remake',\n",
       " 'disney',\n",
       " 'words',\n",
       " 'not',\n",
       " 'explain',\n",
       " 'spectacular',\n",
       " 'go',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'not',\n",
       " 'like',\n",
       " 'stupid',\n",
       " 'pant',\n",
       " 'emma',\n",
       " 'watson',\n",
       " 'wear',\n",
       " 'ufeff',\n",
       " 'movie',\n",
       " 'pretty',\n",
       " 'confuse',\n",
       " 'another',\n",
       " 'wonderful',\n",
       " 'job',\n",
       " 'disney',\n",
       " 'son',\n",
       " 'little',\n",
       " 'scare',\n",
       " 'jafar',\n",
       " 'love',\n",
       " 'fantastic',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'follow',\n",
       " 'original',\n",
       " 'disney',\n",
       " 't',\n",
       " 'spectacular',\n",
       " 'grind',\n",
       " 'break',\n",
       " 'visual',\n",
       " 'good',\n",
       " 'choose',\n",
       " 'cast',\n",
       " 'story',\n",
       " 'depth',\n",
       " 'intelligence',\n",
       " 'jungle',\n",
       " 'book',\n",
       " 'not',\n",
       " 'successful',\n",
       " 'remake',\n",
       " 'arguably',\n",
       " 'improve',\n",
       " 'original',\n",
       " 'cgi',\n",
       " 'fantasticthe',\n",
       " 'voice',\n",
       " 'awesomewe',\n",
       " 'know',\n",
       " 'music',\n",
       " 'story',\n",
       " 'greatplus',\n",
       " 'watch',\n",
       " 'nat',\n",
       " 'geo',\n",
       " 'huge',\n",
       " 'screenquite',\n",
       " 'move',\n",
       " 'stun',\n",
       " 'endear',\n",
       " 'not',\n",
       " 'understand',\n",
       " 'other',\n",
       " 'complain',\n",
       " 'movie',\n",
       " 'great',\n",
       " 'another',\n",
       " 'great',\n",
       " 'disney',\n",
       " 'adaption',\n",
       " 'best',\n",
       " 'flip',\n",
       " 'disney',\n",
       " 'movie',\n",
       " 'focus',\n",
       " 'character',\n",
       " 'main',\n",
       " 'character',\n",
       " 'beautiful',\n",
       " 'addition',\n",
       " 'song',\n",
       " 'original',\n",
       " 'movie',\n",
       " 'feast',\n",
       " 'eye',\n",
       " 'loved',\n",
       " 'stay',\n",
       " 'close',\n",
       " 'original',\n",
       " 'animate',\n",
       " 'movie',\n",
       " 'new',\n",
       " 'song',\n",
       " 'fun',\n",
       " 'modern',\n",
       " 'dance',\n",
       " 'move',\n",
       " 'smith',\n",
       " 'fantastic',\n",
       " 'remake',\n",
       " 'reboot',\n",
       " 'hollywood',\n",
       " 'crawl',\n",
       " 'day',\n",
       " 'especially',\n",
       " 'disney',\n",
       " 'remake',\n",
       " 'animate',\n",
       " 'classic',\n",
       " 'hard',\n",
       " 'find',\n",
       " 'set',\n",
       " 'apart',\n",
       " 'other',\n",
       " 'jungle',\n",
       " 'book',\n",
       " 'good',\n",
       " 'set',\n",
       " 'bar',\n",
       " 'high',\n",
       " 'future',\n",
       " 'remake',\n",
       " 'reboot',\n",
       " 'doe',\n",
       " 'stun',\n",
       " 'fashion',\n",
       " 'jon',\n",
       " 'favreau',\n",
       " 'well',\n",
       " 'know',\n",
       " 'directing',\n",
       " 'ufeff',\n",
       " 'iron',\n",
       " 'man',\n",
       " 'film',\n",
       " 'chef',\n",
       " 'helm',\n",
       " 'bring',\n",
       " 'historic',\n",
       " 'rudyard',\n",
       " 'kipling',\n",
       " 'novel',\n",
       " 'disney',\n",
       " 'animated',\n",
       " 'classic',\n",
       " 'film',\n",
       " 'life',\n",
       " 'live',\n",
       " 'action',\n",
       " 'cgi',\n",
       " 'far',\n",
       " 'superior',\n",
       " 'doe',\n",
       " 'surprisingly',\n",
       " 'good',\n",
       " 'fast',\n",
       " 'pace',\n",
       " 'excite',\n",
       " 'emotional',\n",
       " 'fun',\n",
       " 'stick',\n",
       " 'close',\n",
       " 'root',\n",
       " 'animated',\n",
       " 'classic',\n",
       " 'throw',\n",
       " 'curveballs',\n",
       " 'make',\n",
       " 'somewhat',\n",
       " 'think',\n",
       " 'cool',\n",
       " 'start',\n",
       " 'exact',\n",
       " 'open',\n",
       " 'music',\n",
       " 'animated',\n",
       " 'classic',\n",
       " 'disney',\n",
       " 'castle',\n",
       " 'logo',\n",
       " 'vanish',\n",
       " 'dark',\n",
       " 'decend',\n",
       " 'deep',\n",
       " 'jungle',\n",
       " 'new',\n",
       " 'music',\n",
       " 'hit',\n",
       " 'star',\n",
       " 'appear',\n",
       " 'mad',\n",
       " 'dash',\n",
       " 'think',\n",
       " 'favreau',\n",
       " 'well',\n",
       " 'need',\n",
       " 'musical',\n",
       " 'number',\n",
       " 'film',\n",
       " 'not',\n",
       " 'favorite',\n",
       " 'memorable',\n",
       " 'piece',\n",
       " 'love',\n",
       " 'original',\n",
       " 'good',\n",
       " 'new',\n",
       " 'thing',\n",
       " 'small',\n",
       " 'child',\n",
       " 'find',\n",
       " 'frighten',\n",
       " 'neel',\n",
       " 'sethi',\n",
       " 'new',\n",
       " 'mowgli',\n",
       " 'year',\n",
       " 'old',\n",
       " 'act',\n",
       " 'debut',\n",
       " 'man',\n",
       " 'way',\n",
       " 'start',\n",
       " 'career',\n",
       " 'role',\n",
       " 'bold',\n",
       " 'heroic',\n",
       " 'child',\n",
       " 'story',\n",
       " 'tell',\n",
       " 'great',\n",
       " 'director',\n",
       " 'superb',\n",
       " 'voice',\n",
       " 'cast',\n",
       " 'well',\n",
       " 'cgi',\n",
       " 'date',\n",
       " 'nail',\n",
       " 'role',\n",
       " 'way',\n",
       " 'mowgli',\n",
       " 'want',\n",
       " 'dare',\n",
       " 'brave',\n",
       " 'curious',\n",
       " 'adult',\n",
       " 'feature',\n",
       " 'act',\n",
       " 'like',\n",
       " 'kid',\n",
       " 'impress',\n",
       " 'performance',\n",
       " 'bright',\n",
       " 'future',\n",
       " 'young',\n",
       " 'actor',\n",
       " 'rest',\n",
       " 'cast',\n",
       " 'opinion',\n",
       " 'not',\n",
       " 'appoint',\n",
       " 'well',\n",
       " 'only',\n",
       " 'question',\n",
       " 'scarlet',\n",
       " 'johansson',\n",
       " 'kaa',\n",
       " 'minute',\n",
       " 'screen',\n",
       " 'time',\n",
       " 'change',\n",
       " 'mind',\n",
       " 'seductive',\n",
       " 'endear',\n",
       " 'hypnotize',\n",
       " 'voice',\n",
       " 'giant',\n",
       " 'snake',\n",
       " 'ben',\n",
       " 'kingsley',\n",
       " 'idris',\n",
       " 'elba',\n",
       " 'nail',\n",
       " 'voice',\n",
       " 'role',\n",
       " 'bagheera',\n",
       " 'shere',\n",
       " 'khan',\n",
       " 'wish',\n",
       " 'elba',\n",
       " 'little',\n",
       " 'fierce',\n",
       " 'voice',\n",
       " 'overs',\n",
       " 'good',\n",
       " 'think',\n",
       " 'sound',\n",
       " 'scary',\n",
       " 'big',\n",
       " 'winner',\n",
       " 'bill',\n",
       " 'murray',\n",
       " 'baloo',\n",
       " 'favorite',\n",
       " 'lazy',\n",
       " 'singe',\n",
       " 'trickster',\n",
       " 'bear',\n",
       " 'not',\n",
       " 'claim',\n",
       " 'role',\n",
       " 'authority',\n",
       " 'bill',\n",
       " 'murray',\n",
       " 'no',\n",
       " 'seriousness',\n",
       " 'not',\n",
       " 'think',\n",
       " 'cast',\n",
       " 'well',\n",
       " 'role',\n",
       " 'considering',\n",
       " 'role',\n",
       " 'similar',\n",
       " 'good',\n",
       " 'mentality',\n",
       " 'role',\n",
       " 'anyways',\n",
       " 'overall',\n",
       " 'great',\n",
       " 'film',\n",
       " 'prop',\n",
       " 'favreau',\n",
       " 'hard',\n",
       " 'task',\n",
       " 'come',\n",
       " 'successful',\n",
       " 'watch',\n",
       " 'plan',\n",
       " 'not',\n",
       " 'love',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'go',\n",
       " 'film',\n",
       " 'expect',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'go',\n",
       " 'hard',\n",
       " 'come',\n",
       " 'day',\n",
       " 'especially',\n",
       " 'like',\n",
       " 'story',\n",
       " 'good',\n",
       " 'know',\n",
       " 'cgi',\n",
       " 'fire',\n",
       " 'cylinder',\n",
       " 'make',\n",
       " 'animal',\n",
       " 'look',\n",
       " 'real',\n",
       " 'hint',\n",
       " 'look',\n",
       " 'real',\n",
       " 'peril',\n",
       " 'animal',\n",
       " 'action',\n",
       " 'intense',\n",
       " 'little',\n",
       " 'jungle',\n",
       " 'expect',\n",
       " 'jd',\n",
       " 'animation',\n",
       " 'cool',\n",
       " 'lion',\n",
       " 'no',\n",
       " 'emotion',\n",
       " 'voice',\n",
       " 'not',\n",
       " 'match',\n",
       " 'key',\n",
       " 'line',\n",
       " 'remove',\n",
       " 'pretty',\n",
       " 'not',\n",
       " 'feel',\n",
       " 'reason',\n",
       " 'heart',\n",
       " 'trade',\n",
       " 'visual',\n",
       " 'scary',\n",
       " 'young',\n",
       " 'year',\n",
       " 'old',\n",
       " 'great',\n",
       " 'movie',\n",
       " 'son',\n",
       " 'love',\n",
       " 'movie',\n",
       " 'nothing',\n",
       " 'dislike',\n",
       " 'hate',\n",
       " 'hate',\n",
       " 'movie',\n",
       " 'amaze',\n",
       " 'want',\n",
       " 'sit',\n",
       " 'pick',\n",
       " 'apart',\n",
       " 'guess',\n",
       " 'find',\n",
       " 'way',\n",
       " 'want',\n",
       " 'go',\n",
       " 'beautiful',\n",
       " 'funny',\n",
       " 'movie',\n",
       " 'entertain',\n",
       " 'ufeff',\n",
       " 'minute',\n",
       " 'last',\n",
       " 'certainly',\n",
       " 'animal',\n",
       " 'beautiful',\n",
       " 'not',\n",
       " 'capture',\n",
       " 'emotion',\n",
       " 'original',\n",
       " 'movie',\n",
       " 'joke',\n",
       " 'song',\n",
       " 'cringe',\n",
       " 'smith',\n",
       " 'bad',\n",
       " 'terrible',\n",
       " 'movie',\n",
       " 'watch',\n",
       " 'original',\n",
       " 'loved',\n",
       " 'hard',\n",
       " 'believe',\n",
       " 'cgi',\n",
       " 'music',\n",
       " 'amaze',\n",
       " 'movie',\n",
       " 'great',\n",
       " 'concession',\n",
       " 'way',\n",
       " 'price',\n",
       " 'great',\n",
       " 'movie',\n",
       " 'like',\n",
       " 'detract',\n",
       " 'excessive',\n",
       " 'charge',\n",
       " 'concession',\n",
       " 'extremely',\n",
       " 'good',\n",
       " 'excellent',\n",
       " 'not',\n",
       " 'listen',\n",
       " 'haters',\n",
       " 'go',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'absolutely',\n",
       " 'love',\n",
       " 'movie',\n",
       " 'not',\n",
       " 'give',\n",
       " 'star',\n",
       " 'reason',\n",
       " 'pretty',\n",
       " 'clean',\n",
       " 'wholesome',\n",
       " 'little',\n",
       " 'traditional',\n",
       " 'use',\n",
       " 'right',\n",
       " 'cast',\n",
       " 'not',\n",
       " 'diverse',\n",
       " 'tv',\n",
       " 'adaptation',\n",
       " 'brandy',\n",
       " 'whitney',\n",
       " 'know',\n",
       " 'right',\n",
       " 'away',\n",
       " 'not',\n",
       " 'reach',\n",
       " 'scale',\n",
       " 'overall',\n",
       " 'guess',\n",
       " 'glad',\n",
       " 'go',\n",
       " 'branagh',\n",
       " 'save',\n",
       " 'pointless',\n",
       " 'bear',\n",
       " 'brace',\n",
       " 'great',\n",
       " 'music',\n",
       " 'act',\n",
       " 'love',\n",
       " 'music',\n",
       " 'beautiful',\n",
       " 'remake',\n",
       " 'music',\n",
       " 'fantastic',\n",
       " 'simply',\n",
       " 'amazing',\n",
       " 'not',\n",
       " 'traditional',\n",
       " 'aladdin',\n",
       " 'story',\n",
       " 'hear',\n",
       " 'start',\n",
       " 'end',\n",
       " 'entertainment',\n",
       " 'surprise',\n",
       " 'well',\n",
       " 'movie',\n",
       " 'enjoy',\n",
       " 'disney',\n",
       " 'magic',\n",
       " 'well',\n",
       " 'movie',\n",
       " 'well',\n",
       " 'lion',\n",
       " 'king',\n",
       " 'action',\n",
       " 'comedy',\n",
       " 'good',\n",
       " 'special',\n",
       " 'effect',\n",
       " 'great',\n",
       " 'live',\n",
       " 'action',\n",
       " 'remake',\n",
       " 'not',\n",
       " 'super',\n",
       " 'significant',\n",
       " 'change',\n",
       " 'stage',\n",
       " 'true',\n",
       " 'script',\n",
       " 'year',\n",
       " 'old',\n",
       " 'happy',\n",
       " 'movie',\n",
       " 'let',\n",
       " 'disney',\n",
       " 'movie',\n",
       " 'not',\n",
       " 'remake',\n",
       " 'classic',\n",
       " 'not',\n",
       " 'go',\n",
       " 'give',\n",
       " 'justice',\n",
       " 'entire',\n",
       " 'movie',\n",
       " 'feel',\n",
       " 'rush',\n",
       " 'not',\n",
       " 'cry',\n",
       " 'mufasa',\n",
       " 'die',\n",
       " 'like',\n",
       " 'ruin',\n",
       " 'classic',\n",
       " 'like',\n",
       " 'yikes',\n",
       " 'not',\n",
       " 'realism',\n",
       " 'make',\n",
       " 'depress',\n",
       " 'storyline',\n",
       " 'original',\n",
       " 'destroy',\n",
       " 'original',\n",
       " 'awesome',\n",
       " 'd',\n",
       " 'animation',\n",
       " 'loved',\n",
       " 'great',\n",
       " 'job',\n",
       " 'involve',\n",
       " 'loved',\n",
       " 'movie',\n",
       " 'great',\n",
       " 'special',\n",
       " 'effect',\n",
       " 'music',\n",
       " 'great',\n",
       " 'star',\n",
       " 'great',\n",
       " 'job',\n",
       " 'recreation',\n",
       " 'enjoyed',\n",
       " 'minute',\n",
       " 'movie',\n",
       " 'especially',\n",
       " 'd',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'kid',\n",
       " 'love',\n",
       " 'totally',\n",
       " 'funny',\n",
       " 'expect',\n",
       " 'enjoy',\n",
       " 'nephew',\n",
       " 'niece',\n",
       " 'brought',\n",
       " 'memory',\n",
       " 'original',\n",
       " 'cartoon',\n",
       " 'good',\n",
       " 'graphic',\n",
       " 'daughter',\n",
       " 'original',\n",
       " 'movie',\n",
       " 'go',\n",
       " 'last',\n",
       " 'night',\n",
       " 'wow',\n",
       " 'loved',\n",
       " 'followed',\n",
       " 'close',\n",
       " 'original',\n",
       " 'visually',\n",
       " 'stun',\n",
       " 'great',\n",
       " 'homage',\n",
       " 'classic',\n",
       " 'set',\n",
       " 'piece',\n",
       " 'original',\n",
       " 'cartoon',\n",
       " 'maybe',\n",
       " 'little',\n",
       " 'frighten',\n",
       " 'kid',\n",
       " 'under',\n",
       " 'beautiful',\n",
       " 'film',\n",
       " 'loved',\n",
       " 'never',\n",
       " 'big',\n",
       " 'b',\n",
       " 'tb',\n",
       " 'fan',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'mammoth',\n",
       " 'cast',\n",
       " 'doe',\n",
       " 'good',\n",
       " 'lead',\n",
       " 'film',\n",
       " 'visually',\n",
       " 'heavy',\n",
       " 'cgi',\n",
       " 'tone',\n",
       " 'remember',\n",
       " 'live',\n",
       " 'carbon',\n",
       " 'copy',\n",
       " 'animate',\n",
       " 'movie',\n",
       " 'fun',\n",
       " 'check',\n",
       " 'blog',\n",
       " 'tonight',\n",
       " 'cent',\n",
       " 'thought',\n",
       " 'review',\n",
       " 'amazing',\n",
       " 'critics',\n",
       " 'no',\n",
       " 'clue',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'base',\n",
       " 'movie',\n",
       " 'audience',\n",
       " 'experience',\n",
       " 'not',\n",
       " 'critic',\n",
       " 'amaze',\n",
       " 'movie',\n",
       " 'suberb',\n",
       " 'cgi',\n",
       " 'slight',\n",
       " 'change',\n",
       " 'story',\n",
       " 'detail',\n",
       " 'story',\n",
       " 'obs',\n",
       " 'mark',\n",
       " 'spoiler',\n",
       " 'people',\n",
       " 'not',\n",
       " 'original',\n",
       " 'version',\n",
       " 'first',\n",
       " 'movie',\n",
       " 'enjoyable',\n",
       " 'not',\n",
       " 'bring',\n",
       " 'new',\n",
       " 'table',\n",
       " 'classic',\n",
       " 'story',\n",
       " 'tell',\n",
       " 'amaze',\n",
       " 'photo',\n",
       " 'realistic',\n",
       " 'visual',\n",
       " 'cgi',\n",
       " 'animal',\n",
       " 'easily',\n",
       " 'mistake',\n",
       " 'real',\n",
       " 'not',\n",
       " 'animal',\n",
       " 'ability',\n",
       " 'speak',\n",
       " 'probably',\n",
       " 'think',\n",
       " 'filmmakers',\n",
       " 'film',\n",
       " 'real',\n",
       " 'lion',\n",
       " 'realistic',\n",
       " 'look',\n",
       " 'come',\n",
       " 'cost',\n",
       " 'charcters',\n",
       " 'animal',\n",
       " 'difficult',\n",
       " 'convey',\n",
       " 'emotion',\n",
       " 'fit',\n",
       " 'tone',\n",
       " 'voice',\n",
       " 'scene',\n",
       " 'issue',\n",
       " 'not',\n",
       " 'actor',\n",
       " 'fault',\n",
       " 'voice',\n",
       " 'act',\n",
       " 'fine',\n",
       " 'understable',\n",
       " 'difficult',\n",
       " 'convey',\n",
       " 'facial',\n",
       " 'expression',\n",
       " 'connect',\n",
       " 'human',\n",
       " 'emotion',\n",
       " 'animal',\n",
       " 'completely',\n",
       " 'different',\n",
       " 'facial',\n",
       " 'feature',\n",
       " 'issue',\n",
       " 'lion',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatlist = [word for word in corpus]\n",
    "tokenized = tokenize(str(flatlist))\n",
    "list(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:25:46.985129Z",
     "start_time": "2019-11-14T19:25:46.982772Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(list(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:32:18.583594Z",
     "start_time": "2019-11-14T19:32:18.579948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_corpus = list(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T12:19:02.687588Z",
     "start_time": "2019-11-14T19:32:52.389654Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ef869359bc6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhdpmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHdpModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgensim_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhdpmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/gensim/models/hdpmodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, id2word, max_chunks, max_time, chunksize, kappa, tau, K, T, alpha, gamma, eta, scale, var_converge, outputdir, random_state)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# if a training corpus was provided, start estimating the model right away\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/gensim/models/hdpmodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm_num_docs_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mchunkize_serial\u001b[0;34m(iterable, chunksize, as_numpy, dtype)\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0mwrapped_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m             \u001b[0mwrapped_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hdpmodel = HdpModel(corpus=gensim_corpus, id2word=dictionary)\n",
    "hdpmodel.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()\n",
    "# y_train = label_binarize(y_train,classes=[0,1])\n",
    "from sklearn.utils import multiclass\n",
    "multiclass.type_of_target(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T12:19:02.701753Z",
     "start_time": "2019-11-14T19:35:03.782Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
