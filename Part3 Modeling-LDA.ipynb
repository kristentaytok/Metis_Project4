{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:40:54.805936Z",
     "start_time": "2019-11-13T03:40:54.799988Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gensim\n",
    "\n",
    "#Vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Dimensionality Reduction\n",
    "from sklearn.decomposition import TruncatedSVD #LSA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from gensim import corpora, models, similarities, matutils #LDA\n",
    "\n",
    "#Clustering\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "# !conda install -c districtdatalabs yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "#Word Embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Bayes Optimization Parameter Tuner\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "# import umap\n",
    "# import hdbscan\n",
    "# import sklearn.cluster as cluster\n",
    "\n",
    "\n",
    "# adjectives = []\n",
    "# for sent in hp.sents:\n",
    "#     for word in sent:\n",
    "#         if 'Harry' in word.string:\n",
    "#             for child in word.children:\n",
    "#                 if child.pos_ == 'ADJ': adjectives.append(child.string.strip()) #part of speech         \n",
    "# Counter(adjectives).most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:40:56.014939Z",
     "start_time": "2019-11-13T03:40:55.292550Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('full_df_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:40:56.074073Z",
     "start_time": "2019-11-13T03:40:56.016349Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_df = full_df[full_df['sentiment']=='positive']\n",
    "negative_df = full_df[full_df['sentiment']=='negative']\n",
    "neutral_df = full_df[full_df['sentiment']=='neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline of Vectorization, Dimensionality Reduction, and Clustering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:40:56.083805Z",
     "start_time": "2019-11-13T03:40:56.075544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lionking          20791\n",
       "aladdin           19293\n",
       "beautyandbeast     4571\n",
       "junglebook         3982\n",
       "cinderella         2875\n",
       "Name: movie, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['movie'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:40:56.299474Z",
     "start_time": "2019-11-13T03:40:56.277412Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [items[1] for items in full_df['review_processed'].iteritems()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:40:57.493789Z",
     "start_time": "2019-11-13T03:40:57.489286Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    \"\"\"Function to display topics from Vectorizer after performing Vectorization and Dimensionality Reduction\"\"\"\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:42:03.485301Z",
     "start_time": "2019-11-13T03:40:57.951453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51512, 58)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 0.03974516),\n",
       "  (1, 0.6719355),\n",
       "  (2, 0.04192431),\n",
       "  (3, 0.20762521),\n",
       "  (4, 0.038769882)],\n",
       " [(0, 0.033494614),\n",
       "  (1, 0.6856083),\n",
       "  (2, 0.03349127),\n",
       "  (3, 0.2115014),\n",
       "  (4, 0.03590439)],\n",
       " [(0, 0.067977145),\n",
       "  (1, 0.7286146),\n",
       "  (2, 0.066814244),\n",
       "  (3, 0.06694729),\n",
       "  (4, 0.069646716)],\n",
       " [(0, 0.20920748),\n",
       "  (1, 0.4636509),\n",
       "  (2, 0.043288764),\n",
       "  (3, 0.24291101),\n",
       "  (4, 0.040941782)],\n",
       " [(0, 0.044964857),\n",
       "  (1, 0.7844885),\n",
       "  (2, 0.046877317),\n",
       "  (3, 0.04423541),\n",
       "  (4, 0.07943385)]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvec = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec = tfidfvec.fit_transform(corpus)\n",
    "print(doc_word_tfidfvec.shape)\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus = matutils.Sparse2Corpus(doc_word_tfidfvec.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=gensim_corpus, num_topics=5, id2word=id2word, passes=5)\n",
    "\n",
    "lda.print_topics()\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[gensim_corpus]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]\n",
    "\n",
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:33:39.951112Z",
     "start_time": "2019-11-13T04:33:39.609252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>movie</th>\n",
       "      <th>review_site</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>review_processed</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Disney, WHAT. HAVE. YOU. DONE Just to be clea...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[disney, clear, time, favorite, movie, not, st...</td>\n",
       "      <td>disney clear time favorite movie not stress en...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No soul. The original Lion King is one of my ...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[no, soul, original, lion, king, favorite, mov...</td>\n",
       "      <td>no soul original lion king favorite movie time...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Seriously? So anyone else notice it has a hig...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[seriously, notice, high, score, 7.5, rating, ...</td>\n",
       "      <td>seriously notice high score 7.5 rating not str...</td>\n",
       "      <td>topic3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Overrated and way too much spotlight on beyon...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[overrated, way, spotlight, beyonce, lion, kin...</td>\n",
       "      <td>overrated way spotlight beyonce lion king only...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Terrible acting!! Doesn't compare to the orig...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[terrible, act, not, compare, original, love, ...</td>\n",
       "      <td>terrible act not compare original love origina...</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>A magically wonderful film filled with adventu...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>[magically, wonderful, film, fill, with, adven...</td>\n",
       "      <td>magically wonderful film fill with adventure f...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3041</td>\n",
       "      <td>Disney has overdid the faithfulness of their o...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[disney, overdo, faithfulness, animate, classi...</td>\n",
       "      <td>disney overdo faithfulness animate classic pro...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3042</td>\n",
       "      <td>Magic....that's about right. A re-tell of the ...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[magic, ...., right, tell, original, disney, m...</td>\n",
       "      <td>magic .... right tell original disney movie wi...</td>\n",
       "      <td>topic4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3043</td>\n",
       "      <td>A good movie that sets it apart from the origi...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[good, movie, set, apart, original, story, cin...</td>\n",
       "      <td>good movie set apart original story cinderella...</td>\n",
       "      <td>topic2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3044</td>\n",
       "      <td>It's the Cinderella you know but it's really d...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[cinderella, know, delightfully, charm]</td>\n",
       "      <td>cinderella know delightfully charm</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51512 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_text       movie  \\\n",
       "0      Disney, WHAT. HAVE. YOU. DONE Just to be clea...    lionking   \n",
       "1      No soul. The original Lion King is one of my ...    lionking   \n",
       "5      Seriously? So anyone else notice it has a hig...    lionking   \n",
       "6      Overrated and way too much spotlight on beyon...    lionking   \n",
       "8      Terrible acting!! Doesn't compare to the orig...    lionking   \n",
       "...                                                 ...         ...   \n",
       "3040  A magically wonderful film filled with adventu...  cinderella   \n",
       "3041  Disney has overdid the faithfulness of their o...  cinderella   \n",
       "3042  Magic....that's about right. A re-tell of the ...  cinderella   \n",
       "3043  A good movie that sets it apart from the origi...  cinderella   \n",
       "3044  It's the Cinderella you know but it's really d...  cinderella   \n",
       "\n",
       "         review_site  rating sentiment  \\\n",
       "0               imdb       1  negative   \n",
       "1               imdb       1  negative   \n",
       "5               imdb       1  negative   \n",
       "6               imdb       1  negative   \n",
       "8               imdb       1  negative   \n",
       "...              ...     ...       ...   \n",
       "3040  rottentomatoes       5  positive   \n",
       "3041  rottentomatoes       4  positive   \n",
       "3042  rottentomatoes       4  positive   \n",
       "3043  rottentomatoes       4  positive   \n",
       "3044  rottentomatoes       4  positive   \n",
       "\n",
       "                                          review_tokens  \\\n",
       "0     [disney, clear, time, favorite, movie, not, st...   \n",
       "1     [no, soul, original, lion, king, favorite, mov...   \n",
       "5     [seriously, notice, high, score, 7.5, rating, ...   \n",
       "6     [overrated, way, spotlight, beyonce, lion, kin...   \n",
       "8     [terrible, act, not, compare, original, love, ...   \n",
       "...                                                 ...   \n",
       "3040  [magically, wonderful, film, fill, with, adven...   \n",
       "3041  [disney, overdo, faithfulness, animate, classi...   \n",
       "3042  [magic, ...., right, tell, original, disney, m...   \n",
       "3043  [good, movie, set, apart, original, story, cin...   \n",
       "3044            [cinderella, know, delightfully, charm]   \n",
       "\n",
       "                                       review_processed   topic  \n",
       "0     disney clear time favorite movie not stress en...  topic1  \n",
       "1     no soul original lion king favorite movie time...  topic1  \n",
       "5     seriously notice high score 7.5 rating not str...  topic3  \n",
       "6     overrated way spotlight beyonce lion king only...  topic1  \n",
       "8     terrible act not compare original love origina...  topic0  \n",
       "...                                                 ...     ...  \n",
       "3040  magically wonderful film fill with adventure f...  topic1  \n",
       "3041  disney overdo faithfulness animate classic pro...  topic1  \n",
       "3042  magic .... right tell original disney movie wi...  topic4  \n",
       "3043  good movie set apart original story cinderella...  topic2  \n",
       "3044                 cinderella know delightfully charm  topic1  \n",
       "\n",
       "[51512 rows x 8 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_topic_doc_matrix = pd.DataFrame(lda_docs,columns=['topic0','topic1','topic2','topic3','topic4'])\n",
    "\n",
    "\n",
    "full_topic_doc_matrix['topic0'] = full_topic_doc_matrix['topic0'].str[1].astype(float)\n",
    "full_topic_doc_matrix['topic1'] = full_topic_doc_matrix['topic1'].str[1].astype(float)\n",
    "full_topic_doc_matrix['topic2'] = full_topic_doc_matrix['topic2'].str[1].astype(float)\n",
    "full_topic_doc_matrix['topic3'] = full_topic_doc_matrix['topic3'].str[1].astype(float)\n",
    "full_topic_doc_matrix['topic4'] = full_topic_doc_matrix['topic4'].str[1].astype(float)\n",
    "\n",
    "full_df['topic'] = full_topic_doc_matrix.idxmax(axis=1)\n",
    "full_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:42:03.579893Z",
     "start_time": "2019-11-13T03:40:59.731Z"
    },
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "positive_df = full_df[full_df['sentiment']=='positive']\n",
    "\n",
    "\n",
    "positive_corpus = [items[1] for items in positive_df['review_processed'].iteritems()]\n",
    "\n",
    "tfidfvec_pos = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec_pos = tfidfvec_pos.fit_transform(positive_corpus)\n",
    "print(doc_word_tfidfvec_pos.shape)\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus_pos = matutils.Sparse2Corpus(doc_word_tfidfvec_pos.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec_pos.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=gensim_corpus_pos, num_topics=5, id2word=id2word, passes=5)\n",
    "\n",
    "lda.print_topics()\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus_pos = lda[gensim_corpus_pos]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs_pos = [doc for doc in lda_corpus_pos]\n",
    "\n",
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs_pos[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:16:22.383544Z",
     "start_time": "2019-11-13T04:16:22.183353Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_topic_doc_matrix = pd.DataFrame(lda_docs_pos,columns=['topic0','topic1','topic2','topic3','topic4'])\n",
    "\n",
    "\n",
    "pos_topic_doc_matrix['topic0'] = pos_topic_doc_matrix['topic0'].str[1].astype(float)\n",
    "pos_topic_doc_matrix['topic1'] = pos_topic_doc_matrix['topic1'].str[1].astype(float)\n",
    "pos_topic_doc_matrix['topic2'] = pos_topic_doc_matrix['topic2'].str[1].astype(float)\n",
    "pos_topic_doc_matrix['topic3'] = pos_topic_doc_matrix['topic3'].str[1].astype(float)\n",
    "pos_topic_doc_matrix['topic4'] = pos_topic_doc_matrix['topic4'].str[1].astype(float)\n",
    "\n",
    "positive_df['topic'] = pos_topic_doc_matrix.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:18:45.233428Z",
     "start_time": "2019-11-13T04:18:45.181713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristentokunaga/anaconda3/envs/metis/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "positive_df['topic'] = pos_topic_doc_matrix.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:18:45.303068Z",
     "start_time": "2019-11-13T04:18:45.277404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>movie</th>\n",
       "      <th>review_site</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>review_processed</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>Wonderful Animal Movie 2019 Rating 9.1/10This...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>9</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wonderful, animal, movie, 2019, rating, 9.1/1...</td>\n",
       "      <td>wonderful animal movie 2019 rating 9.1/10this ...</td>\n",
       "      <td>topic4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>Don't listen to the Critics!!! This movie is ...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>10</td>\n",
       "      <td>positive</td>\n",
       "      <td>[not, listen, critics, movie, amazing, imagery...</td>\n",
       "      <td>not listen critics movie amazing imagery color...</td>\n",
       "      <td>topic4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>Great movie The lion king is propably the bes...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>7</td>\n",
       "      <td>positive</td>\n",
       "      <td>[great, movie, lion, king, propably, well, liv...</td>\n",
       "      <td>great movie lion king propably well live actio...</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>Best remake for sure! Don't listen to any cri...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>10</td>\n",
       "      <td>positive</td>\n",
       "      <td>[best, remake, sure, not, listen, critic, peop...</td>\n",
       "      <td>best remake sure not listen critic people like...</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>Timon and Pumba save the world If I'm being h...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>7</td>\n",
       "      <td>positive</td>\n",
       "      <td>[timon, pumba, save, world, honest, well, timo...</td>\n",
       "      <td>timon pumba save world honest well timon pumba...</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>A magically wonderful film filled with adventu...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>5</td>\n",
       "      <td>positive</td>\n",
       "      <td>[magically, wonderful, film, fill, with, adven...</td>\n",
       "      <td>magically wonderful film fill with adventure f...</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3041</td>\n",
       "      <td>Disney has overdid the faithfulness of their o...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[disney, overdo, faithfulness, animate, classi...</td>\n",
       "      <td>disney overdo faithfulness animate classic pro...</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3042</td>\n",
       "      <td>Magic....that's about right. A re-tell of the ...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[magic, ...., right, tell, original, disney, m...</td>\n",
       "      <td>magic .... right tell original disney movie wi...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3043</td>\n",
       "      <td>A good movie that sets it apart from the origi...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[good, movie, set, apart, original, story, cin...</td>\n",
       "      <td>good movie set apart original story cinderella...</td>\n",
       "      <td>topic4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3044</td>\n",
       "      <td>It's the Cinderella you know but it's really d...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>[cinderella, know, delightfully, charm]</td>\n",
       "      <td>cinderella know delightfully charm</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38656 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_text       movie  \\\n",
       "260    Wonderful Animal Movie 2019 Rating 9.1/10This...    lionking   \n",
       "381    Don't listen to the Critics!!! This movie is ...    lionking   \n",
       "469    Great movie The lion king is propably the bes...    lionking   \n",
       "470    Best remake for sure! Don't listen to any cri...    lionking   \n",
       "486    Timon and Pumba save the world If I'm being h...    lionking   \n",
       "...                                                 ...         ...   \n",
       "3040  A magically wonderful film filled with adventu...  cinderella   \n",
       "3041  Disney has overdid the faithfulness of their o...  cinderella   \n",
       "3042  Magic....that's about right. A re-tell of the ...  cinderella   \n",
       "3043  A good movie that sets it apart from the origi...  cinderella   \n",
       "3044  It's the Cinderella you know but it's really d...  cinderella   \n",
       "\n",
       "         review_site  rating sentiment  \\\n",
       "260             imdb       9  positive   \n",
       "381             imdb      10  positive   \n",
       "469             imdb       7  positive   \n",
       "470             imdb      10  positive   \n",
       "486             imdb       7  positive   \n",
       "...              ...     ...       ...   \n",
       "3040  rottentomatoes       5  positive   \n",
       "3041  rottentomatoes       4  positive   \n",
       "3042  rottentomatoes       4  positive   \n",
       "3043  rottentomatoes       4  positive   \n",
       "3044  rottentomatoes       4  positive   \n",
       "\n",
       "                                          review_tokens  \\\n",
       "260   [wonderful, animal, movie, 2019, rating, 9.1/1...   \n",
       "381   [not, listen, critics, movie, amazing, imagery...   \n",
       "469   [great, movie, lion, king, propably, well, liv...   \n",
       "470   [best, remake, sure, not, listen, critic, peop...   \n",
       "486   [timon, pumba, save, world, honest, well, timo...   \n",
       "...                                                 ...   \n",
       "3040  [magically, wonderful, film, fill, with, adven...   \n",
       "3041  [disney, overdo, faithfulness, animate, classi...   \n",
       "3042  [magic, ...., right, tell, original, disney, m...   \n",
       "3043  [good, movie, set, apart, original, story, cin...   \n",
       "3044            [cinderella, know, delightfully, charm]   \n",
       "\n",
       "                                       review_processed   topic  \n",
       "260   wonderful animal movie 2019 rating 9.1/10this ...  topic4  \n",
       "381   not listen critics movie amazing imagery color...  topic4  \n",
       "469   great movie lion king propably well live actio...  topic0  \n",
       "470   best remake sure not listen critic people like...  topic0  \n",
       "486   timon pumba save world honest well timon pumba...  topic0  \n",
       "...                                                 ...     ...  \n",
       "3040  magically wonderful film fill with adventure f...  topic0  \n",
       "3041  disney overdo faithfulness animate classic pro...  topic0  \n",
       "3042  magic .... right tell original disney movie wi...  topic1  \n",
       "3043  good movie set apart original story cinderella...  topic4  \n",
       "3044                 cinderella know delightfully charm  topic0  \n",
       "\n",
       "[38656 rows x 8 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:35:03.938239Z",
     "start_time": "2019-11-13T04:35:03.436248Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_df = full_df[full_df['sentiment']=='negative']\n",
    "\n",
    "negative_corpus = [items[1] for items in negative_df['review_processed'].iteritems()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:36:55.875199Z",
     "start_time": "2019-11-13T04:36:39.742745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5941, 86)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 0.03887234),\n",
       "  (1, 0.03829777),\n",
       "  (2, 0.4030292),\n",
       "  (3, 0.03804812),\n",
       "  (4, 0.4817526)],\n",
       " [(0, 0.030844288),\n",
       "  (1, 0.03179345),\n",
       "  (2, 0.7182067),\n",
       "  (3, 0.030969629),\n",
       "  (4, 0.18818597)],\n",
       " [(0, 0.067958884),\n",
       "  (1, 0.35206753),\n",
       "  (2, 0.06778649),\n",
       "  (3, 0.06850943),\n",
       "  (4, 0.4436777)],\n",
       " [(0, 0.18893284),\n",
       "  (1, 0.03973202),\n",
       "  (2, 0.5104033),\n",
       "  (3, 0.039378364),\n",
       "  (4, 0.22155349)],\n",
       " [(0, 0.81381106),\n",
       "  (1, 0.064637534),\n",
       "  (2, 0.03939757),\n",
       "  (3, 0.041824307),\n",
       "  (4, 0.04032958)]]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvec_neg = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec_neg = tfidfvec_neg.fit_transform(negative_corpus)\n",
    "print(doc_word_tfidfvec_neg.shape)\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus_neg = matutils.Sparse2Corpus(doc_word_tfidfvec_neg.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec_neg.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=gensim_corpus_neg, num_topics=5, id2word=id2word, passes=5)\n",
    "\n",
    "lda.print_topics()\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus_neg = lda[gensim_corpus_neg]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs_neg = [doc for doc in lda_corpus_neg]\n",
    "\n",
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs_neg[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T00:07:55.738640Z",
     "start_time": "2019-11-14T00:07:55.726878Z"
    }
   },
   "outputs": [],
   "source": [
    "tuples_df_neg = pd.DataFrame(lda_docs_neg,columns=['topic1','topic2','topic3','topic4','topic5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T00:09:04.337490Z",
     "start_time": "2019-11-14T00:09:04.298226Z"
    }
   },
   "outputs": [],
   "source": [
    "tuples_df_neg['topic1'] = tuples_df_neg['topic1'].str[1].astype(float)\n",
    "tuples_df_neg['topic2'] = tuples_df_neg['topic2'].str[1].astype(float)\n",
    "tuples_df_neg['topic3'] = tuples_df_neg['topic3'].str[1].astype(float)\n",
    "tuples_df_neg['topic4'] = tuples_df_neg['topic4'].str[1].astype(float)\n",
    "tuples_df_neg['topic5'] = tuples_df_neg['topic5'].str[1].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T00:09:11.474190Z",
     "start_time": "2019-11-14T00:09:11.461562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.038872</td>\n",
       "      <td>0.038298</td>\n",
       "      <td>0.403029</td>\n",
       "      <td>0.038048</td>\n",
       "      <td>0.481753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.030844</td>\n",
       "      <td>0.031793</td>\n",
       "      <td>0.718207</td>\n",
       "      <td>0.030970</td>\n",
       "      <td>0.188186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.067959</td>\n",
       "      <td>0.352068</td>\n",
       "      <td>0.067786</td>\n",
       "      <td>0.068509</td>\n",
       "      <td>0.443678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>0.039732</td>\n",
       "      <td>0.510403</td>\n",
       "      <td>0.039378</td>\n",
       "      <td>0.221553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.813811</td>\n",
       "      <td>0.064638</td>\n",
       "      <td>0.039398</td>\n",
       "      <td>0.041824</td>\n",
       "      <td>0.040330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5936</td>\n",
       "      <td>0.746404</td>\n",
       "      <td>0.063028</td>\n",
       "      <td>0.063126</td>\n",
       "      <td>0.063224</td>\n",
       "      <td>0.064218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5937</td>\n",
       "      <td>0.064560</td>\n",
       "      <td>0.064955</td>\n",
       "      <td>0.741499</td>\n",
       "      <td>0.064811</td>\n",
       "      <td>0.064175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5938</td>\n",
       "      <td>0.565213</td>\n",
       "      <td>0.053171</td>\n",
       "      <td>0.056413</td>\n",
       "      <td>0.273427</td>\n",
       "      <td>0.051777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5939</td>\n",
       "      <td>0.101585</td>\n",
       "      <td>0.592207</td>\n",
       "      <td>0.101725</td>\n",
       "      <td>0.103402</td>\n",
       "      <td>0.101081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>0.595976</td>\n",
       "      <td>0.101053</td>\n",
       "      <td>0.100959</td>\n",
       "      <td>0.100818</td>\n",
       "      <td>0.101194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5941 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic1    topic2    topic3    topic4    topic5\n",
       "0     0.038872  0.038298  0.403029  0.038048  0.481753\n",
       "1     0.030844  0.031793  0.718207  0.030970  0.188186\n",
       "2     0.067959  0.352068  0.067786  0.068509  0.443678\n",
       "3     0.188933  0.039732  0.510403  0.039378  0.221553\n",
       "4     0.813811  0.064638  0.039398  0.041824  0.040330\n",
       "...        ...       ...       ...       ...       ...\n",
       "5936  0.746404  0.063028  0.063126  0.063224  0.064218\n",
       "5937  0.064560  0.064955  0.741499  0.064811  0.064175\n",
       "5938  0.565213  0.053171  0.056413  0.273427  0.051777\n",
       "5939  0.101585  0.592207  0.101725  0.103402  0.101081\n",
       "5940  0.595976  0.101053  0.100959  0.100818  0.101194\n",
       "\n",
       "[5941 rows x 5 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples_df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:38:01.199011Z",
     "start_time": "2019-11-13T04:38:00.874131Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristentokunaga/anaconda3/envs/metis/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>movie</th>\n",
       "      <th>review_site</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>review_processed</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Disney, WHAT. HAVE. YOU. DONE Just to be clea...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[disney, clear, time, favorite, movie, not, st...</td>\n",
       "      <td>disney clear time favorite movie not stress en...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No soul. The original Lion King is one of my ...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[no, soul, original, lion, king, favorite, mov...</td>\n",
       "      <td>no soul original lion king favorite movie time...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Seriously? So anyone else notice it has a hig...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[seriously, notice, high, score, 7.5, rating, ...</td>\n",
       "      <td>seriously notice high score 7.5 rating not str...</td>\n",
       "      <td>topic3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Overrated and way too much spotlight on beyon...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[overrated, way, spotlight, beyonce, lion, kin...</td>\n",
       "      <td>overrated way spotlight beyonce lion king only...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Terrible acting!! Doesn't compare to the orig...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[terrible, act, not, compare, original, love, ...</td>\n",
       "      <td>terrible act not compare original love origina...</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2954</td>\n",
       "      <td>nothing new to add. still think ever after was...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>[nothing, new, add, think, well, adaptation, c...</td>\n",
       "      <td>nothing new add think well adaptation cinderella</td>\n",
       "      <td>topic2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2972</td>\n",
       "      <td>Waste of time.not for kids. Only for old foks</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>[waste, time.not, kid, only, old, foks]</td>\n",
       "      <td>waste time.not kid only old foks</td>\n",
       "      <td>topic0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987</td>\n",
       "      <td>I feel like the real-life incarnation really b...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>[feel, like, real, life, incarnation, bring, h...</td>\n",
       "      <td>feel like real life incarnation bring home rid...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3023</td>\n",
       "      <td>The only performance should be mentioned and m...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>[only, performance, mention, mark, cate, perfo...</td>\n",
       "      <td>only performance mention mark cate performance...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3031</td>\n",
       "      <td>Though this adaptation of Cinderella has a rea...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>[adaptation, cinderella, nice, charm, feel, nu...</td>\n",
       "      <td>adaptation cinderella nice charm feel number c...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5941 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_text       movie  \\\n",
       "0      Disney, WHAT. HAVE. YOU. DONE Just to be clea...    lionking   \n",
       "1      No soul. The original Lion King is one of my ...    lionking   \n",
       "5      Seriously? So anyone else notice it has a hig...    lionking   \n",
       "6      Overrated and way too much spotlight on beyon...    lionking   \n",
       "8      Terrible acting!! Doesn't compare to the orig...    lionking   \n",
       "...                                                 ...         ...   \n",
       "2954  nothing new to add. still think ever after was...  cinderella   \n",
       "2972      Waste of time.not for kids. Only for old foks  cinderella   \n",
       "2987  I feel like the real-life incarnation really b...  cinderella   \n",
       "3023  The only performance should be mentioned and m...  cinderella   \n",
       "3031  Though this adaptation of Cinderella has a rea...  cinderella   \n",
       "\n",
       "         review_site  rating sentiment  \\\n",
       "0               imdb       1  negative   \n",
       "1               imdb       1  negative   \n",
       "5               imdb       1  negative   \n",
       "6               imdb       1  negative   \n",
       "8               imdb       1  negative   \n",
       "...              ...     ...       ...   \n",
       "2954  rottentomatoes       2  negative   \n",
       "2972  rottentomatoes       1  negative   \n",
       "2987  rottentomatoes       2  negative   \n",
       "3023  rottentomatoes       2  negative   \n",
       "3031  rottentomatoes       2  negative   \n",
       "\n",
       "                                          review_tokens  \\\n",
       "0     [disney, clear, time, favorite, movie, not, st...   \n",
       "1     [no, soul, original, lion, king, favorite, mov...   \n",
       "5     [seriously, notice, high, score, 7.5, rating, ...   \n",
       "6     [overrated, way, spotlight, beyonce, lion, kin...   \n",
       "8     [terrible, act, not, compare, original, love, ...   \n",
       "...                                                 ...   \n",
       "2954  [nothing, new, add, think, well, adaptation, c...   \n",
       "2972            [waste, time.not, kid, only, old, foks]   \n",
       "2987  [feel, like, real, life, incarnation, bring, h...   \n",
       "3023  [only, performance, mention, mark, cate, perfo...   \n",
       "3031  [adaptation, cinderella, nice, charm, feel, nu...   \n",
       "\n",
       "                                       review_processed   topic  \n",
       "0     disney clear time favorite movie not stress en...  topic1  \n",
       "1     no soul original lion king favorite movie time...  topic1  \n",
       "5     seriously notice high score 7.5 rating not str...  topic3  \n",
       "6     overrated way spotlight beyonce lion king only...  topic1  \n",
       "8     terrible act not compare original love origina...  topic0  \n",
       "...                                                 ...     ...  \n",
       "2954   nothing new add think well adaptation cinderella  topic2  \n",
       "2972                   waste time.not kid only old foks  topic0  \n",
       "2987  feel like real life incarnation bring home rid...  topic1  \n",
       "3023  only performance mention mark cate performance...  topic1  \n",
       "3031  adaptation cinderella nice charm feel number c...  topic1  \n",
       "\n",
       "[5941 rows x 8 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_topic_doc_matrix = pd.DataFrame(lda_docs,columns=['topic1','topic2','topic3','topic4','topic5'])\n",
    "\n",
    "\n",
    "neg_topic_doc_matrix['topic1'] = neg_topic_doc_matrix['topic1'].str[1].astype(float)\n",
    "neg_topic_doc_matrix['topic2'] = neg_topic_doc_matrix['topic2'].str[1].astype(float)\n",
    "neg_topic_doc_matrix['topic3'] = neg_topic_doc_matrix['topic3'].str[1].astype(float)\n",
    "neg_topic_doc_matrix['topic4'] = neg_topic_doc_matrix['topic4'].str[1].astype(float)\n",
    "neg_topic_doc_matrix['topic5'] = neg_topic_doc_matrix['topic5'].str[1].astype(float)\n",
    "\n",
    "negative_df['topic'] = neg_topic_doc_matrix.idxmax(axis=1)\n",
    "negative_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T04:13:49.114235Z",
     "start_time": "2019-11-14T04:13:48.814042Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:38:34.186688Z",
     "start_time": "2019-11-13T04:38:34.175639Z"
    }
   },
   "outputs": [],
   "source": [
    "neutral_df = full_df[full_df['sentiment']=='neutral']\n",
    "\n",
    "neutral_corpus = [items[1] for items in neutral_df['review_processed'].iteritems()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T05:10:29.803788Z",
     "start_time": "2019-11-13T05:10:12.139968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5941, 86)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 0.33028632),\n",
       "  (1, 0.038276147),\n",
       "  (2, 0.038460325),\n",
       "  (3, 0.5549804),\n",
       "  (4, 0.037996832)],\n",
       " [(0, 0.3130868),\n",
       "  (1, 0.030381668),\n",
       "  (2, 0.031434707),\n",
       "  (3, 0.594429),\n",
       "  (4, 0.030667776)],\n",
       " [(0, 0.5396535),\n",
       "  (1, 0.06744873),\n",
       "  (2, 0.2554234),\n",
       "  (3, 0.069422394),\n",
       "  (4, 0.06805197)],\n",
       " [(0, 0.039305132),\n",
       "  (1, 0.20891151),\n",
       "  (2, 0.43430197),\n",
       "  (3, 0.039110247),\n",
       "  (4, 0.2783711)],\n",
       " [(0, 0.3521825),\n",
       "  (1, 0.039799344),\n",
       "  (2, 0.05374109),\n",
       "  (3, 0.5144035),\n",
       "  (4, 0.039873563)]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvec_neut = TfidfVectorizer(ngram_range=(1,5), min_df=0.05, stop_words=['movie', 'film'])\n",
    "doc_word_tfidfvec_neut = tfidfvec_neut.fit_transform(negative_corpus)\n",
    "print(doc_word_tfidfvec_neut.shape)\n",
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "gensim_corpus_neut = matutils.Sparse2Corpus(doc_word_tfidfvec_neut.transpose())\n",
    "\n",
    "#Map matrix rows to words (tokens)\n",
    "#We need to save a mapping (dict) of row id to word (token) for later use by gensim:\n",
    "id2word = dict((v, k) for k, v in tfidfvec_neut.vocabulary_.items())\n",
    "\n",
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda_neut = models.LdaModel(corpus=gensim_corpus_neut, num_topics=5, id2word=id2word, passes=5)\n",
    "\n",
    "\n",
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus_neut = lda[gensim_corpus_neut]\n",
    "\n",
    "\n",
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs_neut = [doc for doc in lda_corpus_neut]\n",
    "\n",
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs_neut[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T04:40:45.413967Z",
     "start_time": "2019-11-13T04:40:45.097719Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristentokunaga/anaconda3/envs/metis/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>movie</th>\n",
       "      <th>review_site</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_tokens</th>\n",
       "      <th>review_processed</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>Not bad but not great. I thought the CGI was ...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[not, bad, not, great, think, cgi, great, time...</td>\n",
       "      <td>not bad not great think cgi great time emotion...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>Copyright Why they didn't yet give credit to ...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>6</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[copyright, not, give, credit, kimba, white, l...</td>\n",
       "      <td>copyright not give credit kimba white lion???t...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>The same story + CG + blandness = pointless Y...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[story, cg, blandness, pointless, yes, cg, fil...</td>\n",
       "      <td>story cg blandness pointless yes cg film pheno...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>Disappointed It was disappointed to watch it ...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[disappointed, disappoint, watch, with, nothin...</td>\n",
       "      <td>disappointed disappoint watch with nothing new...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>no point Visually it's amazing, for the first...</td>\n",
       "      <td>lionking</td>\n",
       "      <td>imdb</td>\n",
       "      <td>5</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[no, point, visually, amaze, ﻿1, 10, minute, s...</td>\n",
       "      <td>no point visually amaze ﻿1 10 minute story son...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3018</td>\n",
       "      <td>With unlimited card - Sai and Akeela</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[with, unlimited, card, sai, akeela]</td>\n",
       "      <td>with unlimited card sai akeela</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3019</td>\n",
       "      <td>An alright movie. Though if you're looking for...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[alright, movie, look, fresh, not]</td>\n",
       "      <td>alright movie look fresh not</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3022</td>\n",
       "      <td>good but not as good as malificent.</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[good, not, good, malificent]</td>\n",
       "      <td>good not good malificent</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3027</td>\n",
       "      <td>A delightful, beautiful take on the classic fa...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[delightful, beautiful, classic, fairytale, ge...</td>\n",
       "      <td>delightful beautiful classic fairytale genuine...</td>\n",
       "      <td>topic1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3038</td>\n",
       "      <td>Great costume design and strong cast, though t...</td>\n",
       "      <td>cinderella</td>\n",
       "      <td>rottentomatoes</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[great, costume, design, strong, cast, adaptat...</td>\n",
       "      <td>great costume design strong cast adaptation do...</td>\n",
       "      <td>topic3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6915 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            review_text       movie  \\\n",
       "245    Not bad but not great. I thought the CGI was ...    lionking   \n",
       "282    Copyright Why they didn't yet give credit to ...    lionking   \n",
       "422    The same story + CG + blandness = pointless Y...    lionking   \n",
       "423    Disappointed It was disappointed to watch it ...    lionking   \n",
       "427    no point Visually it's amazing, for the first...    lionking   \n",
       "...                                                 ...         ...   \n",
       "3018               With unlimited card - Sai and Akeela  cinderella   \n",
       "3019  An alright movie. Though if you're looking for...  cinderella   \n",
       "3022                good but not as good as malificent.  cinderella   \n",
       "3027  A delightful, beautiful take on the classic fa...  cinderella   \n",
       "3038  Great costume design and strong cast, though t...  cinderella   \n",
       "\n",
       "         review_site  rating sentiment  \\\n",
       "245             imdb       5   neutral   \n",
       "282             imdb       6   neutral   \n",
       "422             imdb       5   neutral   \n",
       "423             imdb       5   neutral   \n",
       "427             imdb       5   neutral   \n",
       "...              ...     ...       ...   \n",
       "3018  rottentomatoes       3   neutral   \n",
       "3019  rottentomatoes       3   neutral   \n",
       "3022  rottentomatoes       3   neutral   \n",
       "3027  rottentomatoes       3   neutral   \n",
       "3038  rottentomatoes       3   neutral   \n",
       "\n",
       "                                          review_tokens  \\\n",
       "245   [not, bad, not, great, think, cgi, great, time...   \n",
       "282   [copyright, not, give, credit, kimba, white, l...   \n",
       "422   [story, cg, blandness, pointless, yes, cg, fil...   \n",
       "423   [disappointed, disappoint, watch, with, nothin...   \n",
       "427   [no, point, visually, amaze, ﻿1, 10, minute, s...   \n",
       "...                                                 ...   \n",
       "3018               [with, unlimited, card, sai, akeela]   \n",
       "3019                 [alright, movie, look, fresh, not]   \n",
       "3022                      [good, not, good, malificent]   \n",
       "3027  [delightful, beautiful, classic, fairytale, ge...   \n",
       "3038  [great, costume, design, strong, cast, adaptat...   \n",
       "\n",
       "                                       review_processed   topic  \n",
       "245   not bad not great think cgi great time emotion...  topic1  \n",
       "282   copyright not give credit kimba white lion???t...  topic1  \n",
       "422   story cg blandness pointless yes cg film pheno...  topic1  \n",
       "423   disappointed disappoint watch with nothing new...  topic1  \n",
       "427   no point visually amaze ﻿1 10 minute story son...  topic1  \n",
       "...                                                 ...     ...  \n",
       "3018                     with unlimited card sai akeela  topic1  \n",
       "3019                       alright movie look fresh not  topic1  \n",
       "3022                           good not good malificent  topic1  \n",
       "3027  delightful beautiful classic fairytale genuine...  topic1  \n",
       "3038  great costume design strong cast adaptation do...  topic3  \n",
       "\n",
       "[6915 rows x 8 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neut_topic_doc_matrix = pd.DataFrame(lda_docs,columns=['topic0','topic1','topic2','topic3','topic4'])\n",
    "\n",
    "\n",
    "neut_topic_doc_matrix['topic0'] = neut_topic_doc_matrix['topic0'].str[1].astype(float)\n",
    "neut_topic_doc_matrix['topic1'] = neut_topic_doc_matrix['topic1'].str[1].astype(float)\n",
    "neut_topic_doc_matrix['topic2'] = neut_topic_doc_matrix['topic2'].str[1].astype(float)\n",
    "neut_topic_doc_matrix['topic3'] = neut_topic_doc_matrix['topic3'].str[1].astype(float)\n",
    "neut_topic_doc_matrix['topic4'] = neut_topic_doc_matrix['topic4'].str[1].astype(float)\n",
    "\n",
    "neutral_df['topic'] = neut_topic_doc_matrix.idxmax(axis=1)\n",
    "neutral_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T05:11:47.903708Z",
     "start_time": "2019-11-13T05:11:47.898369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.076*\"not\" + 0.039*\"animate\" + 0.037*\"live\" + 0.036*\"version\" + 0.035*\"disney\" + 0.033*\"action\" + 0.032*\"with\" + 0.031*\"original\" + 0.030*\"live action\" + 0.030*\"story\" + 0.022*\"new\" + 0.022*\"like\" + 0.022*\"nothing\" + 0.022*\"old\" + 0.021*\"make\" + 0.020*\"money\" + 0.019*\"remake\" + 0.019*\"time\" + 0.017*\"classic\" + 0.017*\"good\"'),\n",
       " (1,\n",
       "  '0.076*\"original\" + 0.053*\"not\" + 0.052*\"cartoon\" + 0.044*\"remake\" + 0.042*\"watch\" + 0.032*\"miss\" + 0.031*\"well\" + 0.028*\"change\" + 0.028*\"no\" + 0.027*\"make\" + 0.025*\"with\" + 0.022*\"visual\" + 0.022*\"voice\" + 0.021*\"go\" + 0.021*\"disney\" + 0.020*\"line\" + 0.020*\"little\" + 0.019*\"king\" + 0.019*\"like\" + 0.019*\"classic\"'),\n",
       " (2,\n",
       "  '0.075*\"good\" + 0.061*\"not good\" + 0.058*\"not\" + 0.052*\"lion\" + 0.049*\"king\" + 0.047*\"original\" + 0.045*\"lion king\" + 0.043*\"lack\" + 0.034*\"emotion\" + 0.033*\"great\" + 0.033*\"animation\" + 0.033*\"flat\" + 0.032*\"fall\" + 0.019*\"voice\" + 0.018*\"feel\" + 0.018*\"cgi\" + 0.017*\"with\" + 0.014*\"no\" + 0.014*\"story\" + 0.012*\"like\"'),\n",
       " (3,\n",
       "  '0.054*\"not\" + 0.047*\"like\" + 0.038*\"feel\" + 0.036*\"bore\" + 0.033*\"character\" + 0.031*\"bad\" + 0.028*\"look\" + 0.027*\"no\" + 0.026*\"with\" + 0.023*\"animal\" + 0.022*\"feel like\" + 0.022*\"act\" + 0.021*\"singe\" + 0.020*\"voice\" + 0.020*\"make\" + 0.020*\"cgi\" + 0.020*\"original\" + 0.018*\"scene\" + 0.018*\"actor\" + 0.017*\"good\"'),\n",
       " (4,\n",
       "  '0.069*\"smith\" + 0.057*\"not\" + 0.051*\"well\" + 0.045*\"genie\" + 0.043*\"aladdin\" + 0.033*\"original\" + 0.030*\"disappoint\" + 0.028*\"terrible\" + 0.026*\"act\" + 0.026*\"good\" + 0.023*\"with\" + 0.023*\"kid\" + 0.023*\"great\" + 0.022*\"music\" + 0.022*\"song\" + 0.022*\"like\" + 0.020*\"think\" + 0.018*\"thing\" + 0.017*\"only\" + 0.015*\"make\"')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_neut.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T05:15:37.095877Z",
     "start_time": "2019-11-13T05:15:37.074665Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-2e7677b1ae7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# print wordcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mshow_wordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_neut\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \"\"\"\n\u001b[0;32m-> 1524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dispatcher'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_document_topics\u001b[0;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m         \u001b[0mtopic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# normalize distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0;31m# make sure the term IDs are ints, otherwise np will get upset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                 \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0;31m# make sure the term IDs are ints, otherwise np will get upset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                 \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        max_words = 200,\n",
    "        max_font_size = 40, \n",
    "        scale = 3,\n",
    "        random_state = 42\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (20, 20))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "# print wordcloud\n",
    "show_wordcloud(lda_neut[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment (Rating) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T22:20:42.506408Z",
     "start_time": "2019-11-11T22:20:42.480224Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:16:36.927120Z",
     "start_time": "2019-11-13T03:16:34.219Z"
    }
   },
   "outputs": [],
   "source": [
    "max_depth_range = list(range(1, 10))\n",
    "n_estimators_range = list(range(50, 300))\n",
    "gamma_range = list(range(0, 5))\n",
    "param_dist = dict(max_depth = max_depth_range, n_estimators = n_estimators_range, gamma = gamma_range)\n",
    "print(param_dist)\n",
    "\n",
    "rand = RandomizedSearchCV(XGBClassifier(random_state=41), param_dist, cv=5, scoring='roc_auc')\n",
    "rand.fit(X_ros_resampled_TRAIN, y_ros_resampled_TRAIN.values.ravel())\n",
    "rand.cv_results_\n",
    "\n",
    "print(rand.best_score_)\n",
    "print(rand.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:16:36.928021Z",
     "start_time": "2019-11-13T03:16:34.221Z"
    }
   },
   "outputs": [],
   "source": [
    "#RANDOM Oversampling\n",
    "#print(X_train.columns)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_ros_resampled_subtrain, y_ros_resampled_subtrain = ros.fit_sample(X_subtrain,y_subtrain)\n",
    "X_ros_resampled_subtrain = pd.DataFrame(X_ros_resampled_subtrain, columns = ['Administrative',  'Administrative_Duration',\n",
    " 'Informational',  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',  \n",
    " 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Weekend',\n",
    " 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_Jun', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep',\n",
    " 'VisitorType_New_Visitor',  'VisitorType_Returning_Visitor', 'OperatingSystem_1', 'OperatingSystem_2',\n",
    " 'OperatingSystem_3', 'OperatingSystem_4', 'OperatingSystem_6', 'OperatingSystem_7', 'OperatingSystem_8',\n",
    " 'Browser_1', 'Browser_2', 'Browser_3', 'Browser_4', 'Browser_5', 'Browser_6', 'Browser_7', 'Browser_8', 'Browser_10',\n",
    " 'Browser_11', 'Browser_12', 'Browser_13', 'Region_1', 'Region_2', 'Region_3', 'Region_4', 'Region_6', 'Region_7', 'Region_8', 'Region_9', \n",
    " 'TrafficType_1', 'TrafficType_2', 'TrafficType_3', 'TrafficType_4', 'TrafficType_5', 'TrafficType_6', 'TrafficType_7',\n",
    " 'TrafficType_8', 'TrafficType_9', 'TrafficType_10', 'TrafficType_11', 'TrafficType_12', 'TrafficType_13', 'TrafficType_14',\n",
    " 'TrafficType_15', 'TrafficType_16', 'TrafficType_18', 'TrafficType_19', 'TrafficType_20'])\n",
    "y_ros_resampled_subtrain = pd.DataFrame(y_ros_resampled_subtrain, columns = ['Purchase'])\n",
    "y_ros_resampled_subtrain\n",
    "\n",
    "#SMOTE Oversampling\n",
    "X_smoted_subtrain, y_smoted_subtrain = SMOTE(random_state=0).fit_sample(X_subtrain,y_subtrain)\n",
    "X_smoted_subtrain = pd.DataFrame(X_smoted_subtrain, columns = ['Administrative',  'Administrative_Duration',\n",
    " 'Informational',  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',  \n",
    " 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Weekend',\n",
    " 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_Jun', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep',\n",
    " 'VisitorType_New_Visitor',  'VisitorType_Returning_Visitor', 'OperatingSystem_1', 'OperatingSystem_2',\n",
    " 'OperatingSystem_3', 'OperatingSystem_4', 'OperatingSystem_6', 'OperatingSystem_7', 'OperatingSystem_8',\n",
    " 'Browser_1', 'Browser_2', 'Browser_3', 'Browser_4', 'Browser_5', 'Browser_6', 'Browser_7', 'Browser_8', 'Browser_10',\n",
    " 'Browser_11', 'Browser_12', 'Browser_13', 'Region_1', 'Region_2', 'Region_3', 'Region_4', 'Region_6', 'Region_7', 'Region_8', 'Region_9', \n",
    " 'TrafficType_1', 'TrafficType_2', 'TrafficType_3', 'TrafficType_4', 'TrafficType_5', 'TrafficType_6', 'TrafficType_7',\n",
    " 'TrafficType_8', 'TrafficType_9', 'TrafficType_10', 'TrafficType_11', 'TrafficType_12', 'TrafficType_13', 'TrafficType_14',\n",
    " 'TrafficType_15', 'TrafficType_16', 'TrafficType_18', 'TrafficType_19', 'TrafficType_20'])\n",
    "y_smoted_subtrain = pd.DataFrame(y_smoted_subtrain, columns = ['Purchase'])\n",
    "y_smoted_subtrain\n",
    "\n",
    "#ADASYN Oversampling\n",
    "X_adasyn_subtrain, y_adasyn_subtrain = ADASYN(random_state=0).fit_sample(X_subtrain,y_subtrain)\n",
    "X_adasyn_subtrain = pd.DataFrame(X_adasyn_subtrain, columns = ['Administrative',  'Administrative_Duration',\n",
    " 'Informational',  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',  \n",
    " 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Weekend',\n",
    " 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_Jun', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep',\n",
    " 'VisitorType_New_Visitor',  'VisitorType_Returning_Visitor', 'OperatingSystem_1', 'OperatingSystem_2',\n",
    " 'OperatingSystem_3', 'OperatingSystem_4', 'OperatingSystem_6', 'OperatingSystem_7', 'OperatingSystem_8',\n",
    " 'Browser_1', 'Browser_2', 'Browser_3', 'Browser_4', 'Browser_5', 'Browser_6', 'Browser_7', 'Browser_8', 'Browser_10',\n",
    " 'Browser_11', 'Browser_12', 'Browser_13', 'Region_1', 'Region_2', 'Region_3', 'Region_4', 'Region_6', 'Region_7', 'Region_8', 'Region_9', \n",
    " 'TrafficType_1', 'TrafficType_2', 'TrafficType_3', 'TrafficType_4', 'TrafficType_5', 'TrafficType_6', 'TrafficType_7',\n",
    " 'TrafficType_8', 'TrafficType_9', 'TrafficType_10', 'TrafficType_11', 'TrafficType_12', 'TrafficType_13', 'TrafficType_14',\n",
    " 'TrafficType_15', 'TrafficType_16', 'TrafficType_18', 'TrafficType_19', 'TrafficType_20'])\n",
    "y_adasyn_subtrain = pd.DataFrame(y_adasyn_subtrain, columns = ['Purchase'])\n",
    "y_adasyn_subtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:16:36.928830Z",
     "start_time": "2019-11-13T03:16:34.224Z"
    }
   },
   "outputs": [],
   "source": [
    "#All models - Imbalanced\n",
    "Model_Results = pd.DataFrame(columns = ['Model','Fbeta1.5','Fbeta2', 'F1','Precision','Recall', 'P-R_AUC', 'ROC_AUC_train','ROC_AUC_valid','FN','FP','TN','TP','Fit','Predict','Feature_Importance'])\n",
    "Model_Results['Model'] = ['Dummy_Clas','KNN_Clas_3n','KNN_Clas_5n','KNN_Clas_7n', 'KNN_Clas_3n_weightdist', 'KNN_Clas_3n_Manhattandist','GaussianNB','LogisticRegression','LogisticRegressionCV','SVC',\\\n",
    "                          'DecisionTree_Clas','BalRF_Clas_50est_depth2','BalRF_Clas_100est_depth2','BalRF_Clas_50est_depth3','RF_Clas_50est_depth2','RF_Clas_100est_depth2','RF_Clas_200est_depth2','RF_Clas_100est_depth3','RF_Clas_100est_depth4',\\\n",
    "                          'XGB_Clas_95est_depth2','XGB_Clas_Scale_95est_depth2_PosWt', 'XGB_Clas_100est_depth2','XGB_Clas_50est_depth2','XGB_Clas_95est_depth3','XGB_Clas_95est_depth4','XGB_Clas_95est_depth3_PosWt','XGB_Clas_Scale_100est_depth2_PosWt']\n",
    "models = DummyClassifier(random_state=41), KNeighborsClassifier(n_neighbors=3), KNeighborsClassifier(n_neighbors=5), KNeighborsClassifier(n_neighbors=7), KNeighborsClassifier(n_neighbors=3, weights='distance'),KNeighborsClassifier(n_neighbors=3, p=1), \\\n",
    "        GaussianNB(), LogisticRegression(C=1.0,random_state=41, max_iter=10000), LogisticRegressionCV(Cs=[100000,10000,1000,100,10,1,0.1,0.01,0.001],random_state=41,cv=5,max_iter=10000), SVC(random_state=41,probability=True), DecisionTreeClassifier(random_state=41), \\\n",
    "        BalancedRandomForestClassifier(n_estimators=50, max_depth=2,random_state=41), BalancedRandomForestClassifier(n_estimators=100, max_depth=2,random_state=41), BalancedRandomForestClassifier(n_estimators=50, max_depth=3,random_state=41), RandomForestClassifier(n_estimators=50, max_depth=2,random_state=41), RandomForestClassifier(n_estimators=100, max_depth=2,random_state=41), RandomForestClassifier(n_estimators=200, max_depth=2,random_state=41), RandomForestClassifier(n_estimators=100, max_depth=3,random_state=41),RandomForestClassifier(n_estimators=100, max_depth=4,random_state=41), \\\n",
    "        XGBClassifier(max_depth=2,n_estimators=95,random_state=41), XGBClassifier(max_depth=2,n_estimators=95,random_state=41,scale_pos_weight=(y_subtrain.values==0).sum()/(y_subtrain.values==1).sum()), XGBClassifier(max_depth=2,n_estimators=100,random_state=41), XGBClassifier(max_depth=2,n_estimators=50,random_state=41), XGBClassifier(max_depth=3,n_estimators=95,random_state=41),XGBClassifier(max_depth=4,n_estimators=95,random_state=41), XGBClassifier(max_depth=3,n_estimators=95,random_state=41,scale_pos_weight=(y_subtrain.values==0).sum()/(y_subtrain.values==1).sum()), XGBClassifier(max_depth=2,n_estimators=100,random_state=41,scale_pos_weight=(y_subtrain.values==0).sum()/(y_subtrain.values==1).sum())\n",
    "\n",
    "fbeta1_5_list = []\n",
    "fbeta2_list = []\n",
    "f1_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "PR_AUC_list = []\n",
    "ROC_AUC_train_list = []\n",
    "ROC_AUC_valid_list = []\n",
    "FN_list = []\n",
    "FP_list = []\n",
    "TN_list = []\n",
    "TP_list = []\n",
    "Fit_list = []\n",
    "Predict_list = []\n",
    "Feature_Importance_list = []\n",
    "\n",
    "\n",
    "#std scale train and test: recommended workflow \n",
    "#scaler.fit x-->model.fit --> scaler transform x\n",
    "#model.fit, scaler transform xtest \n",
    "#model learning from data training \n",
    "#scaled test --> leaks data\n",
    "#train on trainset --if input data \n",
    "\n",
    "#pipeline --always gets this right\n",
    "#scaler.fitx\n",
    "#model.fit(scaler.transform(x))\n",
    "#model.predict(scaler.transform(xtest)))\n",
    "#stdscale target can do, but affects betas: less interpretable\n",
    "#similar impact has similar beta \n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    if 'CV' in str(model): \n",
    "        scalerlogregCV = StandardScaler().fit(X_train)\n",
    "        X_train_transformed = scalerlogregCV.transform(X_train.values)\n",
    "        X_train_transformed = pd.DataFrame(X_train_transformed, columns = ['Administrative',  'Administrative_Duration',\n",
    " 'Informational',  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',  \n",
    " 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Weekend',\n",
    " 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_Jun', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep',\n",
    " 'VisitorType_New_Visitor',  'VisitorType_Returning_Visitor', 'OperatingSystem_1', 'OperatingSystem_2',\n",
    " 'OperatingSystem_3', 'OperatingSystem_4', 'OperatingSystem_6', 'OperatingSystem_7', 'OperatingSystem_8',\n",
    " 'Browser_1', 'Browser_2', 'Browser_3', 'Browser_4', 'Browser_5', 'Browser_6', 'Browser_7', 'Browser_8', 'Browser_10',\n",
    " 'Browser_11', 'Browser_12', 'Browser_13', 'Region_1', 'Region_2', 'Region_3', 'Region_4', 'Region_6', 'Region_7', 'Region_8', 'Region_9', \n",
    " 'TrafficType_1', 'TrafficType_2', 'TrafficType_3', 'TrafficType_4', 'TrafficType_5', 'TrafficType_6', 'TrafficType_7',\n",
    " 'TrafficType_8', 'TrafficType_9', 'TrafficType_10', 'TrafficType_11', 'TrafficType_12', 'TrafficType_13', 'TrafficType_14',\n",
    " 'TrafficType_15', 'TrafficType_16', 'TrafficType_18', 'TrafficType_19', 'TrafficType_20'])\n",
    "        model.fit(X_train_transformed,y_train) #turn back into df\n",
    "        \n",
    "        fbeta1_5_list.append(fbeta_score(y_train, model.predict(X_train_transformed),1.5))\n",
    "        fbeta2_list.append(fbeta_score(y_train, model.predict(X_train_transformed),2))\n",
    "        f1_list.append(f1_score(y_train, model.predict(X_train_transformed)))\n",
    "        precision_list.append(precision_score(y_train, model.predict(X_train_transformed)))\n",
    "        recall_list.append(recall_score(y_train, model.predict(X_train_transformed)))\n",
    "        precision, recall, thresholds = precision_recall_curve(y_train, model.predict(X_train_transformed)) \n",
    "        PR_AUC_list.append(auc(recall, precision)) \n",
    "        ROC_AUC_train_list.append(roc_auc_score(y_train, model.predict(X_train_transformed)))\n",
    "        ROC_AUC_valid_list.append(roc_auc_score(y_train, model.predict(X_train_transformed)))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_train, model.predict(X_train_transformed)).ravel()\n",
    "        FN_list.append(fn)\n",
    "        FP_list.append(fp)\n",
    "        TN_list.append(tn)\n",
    "        TP_list.append(tp)\n",
    "        Fit_list.append('X_train_transformed, y_train')\n",
    "        Predict_list.append('X_train_transformed')\n",
    "        Feature_Importance_list.append(model.coef_)\n",
    "            \n",
    "    elif 'LogisticRegression' in str(model):\n",
    "        scalerlogreg = StandardScaler().fit(X_subtrain)\n",
    "        X_subtrain_transformed = scalerlogreg.transform(X_subtrain.values)\n",
    "        X_subtrain_transformed = pd.DataFrame(X_subtrain_transformed, columns = ['Administrative',  'Administrative_Duration',\n",
    " 'Informational',  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',  \n",
    " 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Weekend',\n",
    " 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_Jun', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep',\n",
    " 'VisitorType_New_Visitor',  'VisitorType_Returning_Visitor', 'OperatingSystem_1', 'OperatingSystem_2',\n",
    " 'OperatingSystem_3', 'OperatingSystem_4', 'OperatingSystem_6', 'OperatingSystem_7', 'OperatingSystem_8',\n",
    " 'Browser_1', 'Browser_2', 'Browser_3', 'Browser_4', 'Browser_5', 'Browser_6', 'Browser_7', 'Browser_8', 'Browser_10',\n",
    " 'Browser_11', 'Browser_12', 'Browser_13', 'Region_1', 'Region_2', 'Region_3', 'Region_4', 'Region_6', 'Region_7', 'Region_8', 'Region_9', \n",
    " 'TrafficType_1', 'TrafficType_2', 'TrafficType_3', 'TrafficType_4', 'TrafficType_5', 'TrafficType_6', 'TrafficType_7',\n",
    " 'TrafficType_8', 'TrafficType_9', 'TrafficType_10', 'TrafficType_11', 'TrafficType_12', 'TrafficType_13', 'TrafficType_14',\n",
    " 'TrafficType_15', 'TrafficType_16', 'TrafficType_18', 'TrafficType_19', 'TrafficType_20'])\n",
    "        model.fit(X_subtrain_transformed,y_subtrain) #need to turn back into df\n",
    "        X_valid_transformed = scalerlogreg.transform(X_valid)\n",
    "        X_valid_transformed = pd.DataFrame(X_valid_transformed, columns = ['Administrative',  'Administrative_Duration',\n",
    " 'Informational',  'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration',  \n",
    " 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Weekend',\n",
    " 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_Jun', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep',\n",
    " 'VisitorType_New_Visitor',  'VisitorType_Returning_Visitor', 'OperatingSystem_1', 'OperatingSystem_2',\n",
    " 'OperatingSystem_3', 'OperatingSystem_4', 'OperatingSystem_6', 'OperatingSystem_7', 'OperatingSystem_8',\n",
    " 'Browser_1', 'Browser_2', 'Browser_3', 'Browser_4', 'Browser_5', 'Browser_6', 'Browser_7', 'Browser_8', 'Browser_10',\n",
    " 'Browser_11', 'Browser_12', 'Browser_13', 'Region_1', 'Region_2', 'Region_3', 'Region_4', 'Region_6', 'Region_7', 'Region_8', 'Region_9', \n",
    " 'TrafficType_1', 'TrafficType_2', 'TrafficType_3', 'TrafficType_4', 'TrafficType_5', 'TrafficType_6', 'TrafficType_7',\n",
    " 'TrafficType_8', 'TrafficType_9', 'TrafficType_10', 'TrafficType_11', 'TrafficType_12', 'TrafficType_13', 'TrafficType_14',\n",
    " 'TrafficType_15', 'TrafficType_16', 'TrafficType_18', 'TrafficType_19', 'TrafficType_20'])\n",
    "        fbeta1_5_list.append(fbeta_score(y_valid, model.predict(X_valid_transformed),1.5))\n",
    "        fbeta2_list.append(fbeta_score(y_valid, model.predict(X_valid_transformed),2))\n",
    "        f1_list.append(f1_score(y_valid, model.predict(X_valid_transformed)))\n",
    "        precision_list.append(precision_score(y_valid, model.predict(X_valid_transformed)))\n",
    "        recall_list.append(recall_score(y_valid, model.predict(X_valid_transformed)))\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid, model.predict(X_valid_transformed))\n",
    "        PR_AUC_list.append(auc(recall, precision))\n",
    "        ROC_AUC_train_list.append(roc_auc_score(y_subtrain, model.predict(X_subtrain_transformed)))\n",
    "        ROC_AUC_valid_list.append(roc_auc_score(y_valid, model.predict(X_valid_transformed)))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_valid, model.predict(X_valid_transformed)).ravel()\n",
    "        FN_list.append(fn)\n",
    "        FP_list.append(fp)\n",
    "        TN_list.append(tn)\n",
    "        TP_list.append(tp)\n",
    "        Fit_list.append('X_subtrain_transformed, y_subtrain')\n",
    "        Predict_list.append('X_valid_transformed')\n",
    "        Feature_Importance_list.append(model.coef_)\n",
    "    \n",
    "    elif 'DummyClassifier' in str(model) or 'KNeighborsClassifier' in str(model) or 'GaussianNB' in str(model) or 'SVC' in str(model):\n",
    "        model.fit(X_subtrain,y_subtrain)\n",
    "        fbeta1_5_list.append(fbeta_score(y_valid, model.predict(X_valid),1.5))\n",
    "        fbeta2_list.append(fbeta_score(y_valid, model.predict(X_valid),2))\n",
    "        f1_list.append(f1_score(y_valid, model.predict(X_valid)))\n",
    "        precision_list.append(precision_score(y_valid, model.predict(X_valid)))\n",
    "        recall_list.append(recall_score(y_valid, model.predict(X_valid)))\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid, model.predict(X_valid))\n",
    "        PR_AUC_list.append(auc(recall, precision))\n",
    "        ROC_AUC_train_list.append(roc_auc_score(y_subtrain, model.predict(X_subtrain)))\n",
    "        ROC_AUC_valid_list.append(roc_auc_score(y_valid, model.predict(X_valid)))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_valid, model.predict(X_valid)).ravel()\n",
    "        FN_list.append(fn)\n",
    "        FP_list.append(fp)\n",
    "        TN_list.append(tn)\n",
    "        TP_list.append(tp)\n",
    "        Fit_list.append('X_subtrain, y_subtrain')\n",
    "        Predict_list.append('X_valid')\n",
    "        Feature_Importance_list.append('N/A')\n",
    "        \n",
    "    else:\n",
    "        model.fit(X_subtrain,y_subtrain)\n",
    "        fbeta1_5_list.append(fbeta_score(y_valid, model.predict(X_valid),1.5))\n",
    "        fbeta2_list.append(fbeta_score(y_valid, model.predict(X_valid),2))\n",
    "        f1_list.append(f1_score(y_valid, model.predict(X_valid)))\n",
    "        precision_list.append(precision_score(y_valid, model.predict(X_valid)))\n",
    "        recall_list.append(recall_score(y_valid, model.predict(X_valid)))\n",
    "        precision, recall, thresholds = precision_recall_curve(y_valid, model.predict(X_valid))\n",
    "        PR_AUC_list.append(auc(recall, precision))\n",
    "        ROC_AUC_train_list.append(roc_auc_score(y_subtrain, model.predict(X_subtrain)))\n",
    "        ROC_AUC_valid_list.append(roc_auc_score(y_valid, model.predict(X_valid))) #technically, decision tree models should use OOB error to identify overfitting, but can't compare to other models\n",
    "        tn, fp, fn, tp = confusion_matrix(y_valid, model.predict(X_valid)).ravel()\n",
    "        FN_list.append(fn)\n",
    "        FP_list.append(fp)\n",
    "        TN_list.append(tn)\n",
    "        TP_list.append(tp)\n",
    "        Fit_list.append('X_subtrain, y_subtrain')\n",
    "        Predict_list.append('X_valid')\n",
    "        Feature_Importance_list.append(model.feature_importances_)\n",
    "        \n",
    "Model_Results['Fbeta1.5'] = fbeta1_5_list\n",
    "Model_Results['Fbeta2'] = fbeta2_list\n",
    "Model_Results['F1'] = f1_list\n",
    "Model_Results['Precision'] = precision_list\n",
    "Model_Results['Recall'] = recall_list\n",
    "Model_Results['P-R_AUC'] = PR_AUC_list\n",
    "Model_Results['ROC_AUC_train'] = ROC_AUC_train_list\n",
    "Model_Results['ROC_AUC_valid'] = ROC_AUC_valid_list   \n",
    "Model_Results['FN'] = FN_list\n",
    "Model_Results['FP'] = FP_list\n",
    "Model_Results['TN'] = TN_list\n",
    "Model_Results['TP'] = TP_list\n",
    "Model_Results['Fit'] = Fit_list\n",
    "Model_Results['Predict'] = Predict_list\n",
    "Model_Results['Feature_Importance'] = Feature_Importance_list\n",
    "Model_Results.sort_values(['FN', 'ROC_AUC_valid'], ascending=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T03:16:36.929623Z",
     "start_time": "2019-11-13T03:16:34.227Z"
    }
   },
   "outputs": [],
   "source": [
    "Merge_Models = pd.concat([Model_Results, Model_Results_ROS, Model_Results_SMOTE, Model_Results_ADASYN],sort=False)\n",
    "Merge_Models.sort_values(['ROC_AUC_valid','FN'], ascending=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T14:17:26.104143Z",
     "start_time": "2019-11-12T14:14:27.188Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
